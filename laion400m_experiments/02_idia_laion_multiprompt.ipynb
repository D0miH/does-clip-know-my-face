{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('../')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "\n",
    "import torch\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset as PTConcatDataset\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import entropy\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import os\n",
    "import bisect\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "from rtpt.rtpt import setproctitle\n",
    "setproctitle('@Clip_Notebook')\n",
    "\n",
    "from datasets import FaceScrub, SingleClassSubset\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 15)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "MODEL_NAMES = ['ViT-B-32', 'ViT-B-16', 'ViT-L-14']\n",
    "NUM_TOTAL_NAMES = 1_000\n",
    "PROMPTS = [\n",
    "    '{0}',\n",
    "    'an image of {0}', \n",
    "    'a photo of {0}', \n",
    "    '{0} on a photo', \n",
    "    'a photo of a person named {0}', \n",
    "    'a person named {0}', \n",
    "    'a man named {0}',\n",
    "    'a woman named {0}',\n",
    "    'the name of the person is {0}', \n",
    "    'a photo of a person with the name {0}', \n",
    "    '{0} at a gala', \n",
    "    'a photo of the celebrity {0}', \n",
    "    'actor {0}',\n",
    "    'actress {0}',\n",
    "    'a colored photo of {0}',\n",
    "    'a black and white photo of {0}',\n",
    "    'a cool photo of {0}',\n",
    "    'a cropped photo of {0}',\n",
    "    'a cropped image of {0}',\n",
    "    '{0} in a suit',\n",
    "    '{0} in a dress'\n",
    "]\n",
    "SEED = 42\n",
    "MIN_NUM_IMAGES_AVAILABLE = 30 # the number of samples for a person that need to be available in order to consider it in the experiments. Persons with less will not be included in the experiments.\n",
    "MAX_NUM_TRAINING_SAMPLES = 300 # the maximum number of samples of an individual in the training set to be considered for the experiments\n",
    "MIN_NUM_CORRECT_PROMPT_PREDS = 1 # the number of prompts for which the majority prediction has to be correct (tau in the paper)\n",
    "\n",
    "LOAD_PREDICTIONS_FROM_FILE = True\n",
    "LOAD_PREDICTION_METRICS_FROM_FILE = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init clip\n",
    "models = {}\n",
    "preprocessings = {}\n",
    "for model_name in MODEL_NAMES:\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained='laion400m_e32')\n",
    "    preprocessings[model_name] = preprocess\n",
    "    model = model.eval()\n",
    "    models[model_name] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get the predictions for an actor/actress\n",
    "@torch.no_grad()\n",
    "def get_text_embeddings(model, context, context_batchsize=10_000, use_tqdm=False):\n",
    "    context_batchsize = context_batchsize * torch.cuda.device_count()\n",
    "    # if there is not batches for the context unsqueeze it\n",
    "    if context.dim() < 3:\n",
    "        context = context.unsqueeze(0)\n",
    "\n",
    "    # get the batch size, the number of labels and the sequence length\n",
    "    seq_len = context.shape[-1]\n",
    "    viewed_context = context.view(-1, seq_len)\n",
    "\n",
    "    text_features = []\n",
    "    for context_batch_idx in tqdm(range(0, len(viewed_context), context_batchsize), desc=\"Calculating Text Embeddings\", disable=not use_tqdm):\n",
    "        context_batch = viewed_context[context_batch_idx:context_batch_idx + context_batchsize]\n",
    "        batch_text_features= model.encode_text(context_batch, normalize=True).cpu()\n",
    "\n",
    "        text_features.append(batch_text_features)\n",
    "    text_features = torch.cat(text_features).view(list(context.shape[:-1]) + [-1])\n",
    "\n",
    "    return text_features\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_preds_for_dataset(model, subset, context, batch_size=8, num_workers=8, device=device, context_batchsize=10_000, no_tqdm=False, text_embeddings=None):\n",
    "    dataloader = DataLoader(subset, batch_size=batch_size, num_workers=num_workers, pin_memory=device == 'cuda')\n",
    "\n",
    "    if text_embeddings is None:\n",
    "        text_embeddings = get_text_embeddings(model, context, context_batchsize=context_batchsize)\n",
    "\n",
    "    preds = []\n",
    "    for x, _ in tqdm(dataloader, desc='Iterating Dataset', disable=no_tqdm):\n",
    "        x = x.to(device)\n",
    "        image_features = model.encode_image(x, normalize=True).cpu()\n",
    "\n",
    "        image_features = image_features.unsqueeze(0)\n",
    "\n",
    "        # we have to calculate the cosine similarity manually. OpenAI does this internally.\n",
    "        logits_per_image = model.logit_scale.exp().cpu()  * image_features @ text_embeddings.swapaxes(-1, -2)\n",
    "        preds.append(logits_per_image.argmax(-1))\n",
    "\n",
    "    return torch.cat(preds, dim=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data to be able to concatenate the facescrub dataset with the dataset containing the european individuals\n",
    "class ConcatDataset(PTConcatDataset):\n",
    "    @property\n",
    "    def classes(self):\n",
    "        classes = []\n",
    "        for dataset in self.datasets:\n",
    "            classes.extend(dataset.classes)\n",
    "\n",
    "        return classes\n",
    "\n",
    "    @property\n",
    "    def class_to_idx(self):\n",
    "        return {name: i for i, name in enumerate(self.classes)}\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        targets = []\n",
    "        for i, dataset in enumerate(self.datasets):\n",
    "            max_target = sum([len(self.datasets[i].classes) for i in range(0, i)])\n",
    "            targets.extend((np.array(dataset.targets) + max_target).tolist())\n",
    "        \n",
    "        return targets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0:\n",
    "            if -idx > len(self):\n",
    "                raise ValueError(\"absolute value of index should not exceed dataset length\")\n",
    "            idx = len(self) + idx\n",
    "        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n",
    "        if dataset_idx == 0:\n",
    "            sample_idx = idx\n",
    "        else:\n",
    "            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n",
    "\n",
    "        x, y = self.datasets[dataset_idx][sample_idx]\n",
    "\n",
    "        max_target = sum([len(self.datasets[i].classes) for i in range(0, dataset_idx)])\n",
    "\n",
    "        return x, y + max_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facescrub_datasets = FaceScrub(root='./data/facescrub', group='all', train=True, cropped=False, transform=preprocess)\n",
    "print(f'FaceScrub Dataset size: {len(facescrub_datasets)}')\n",
    "print(f'FaceScrub Total Number of Classes: {len(facescrub_datasets.classes)}')\n",
    "\n",
    "laion_european_actors = ImageFolder('./data/laion_european_celebs/actors/images', transform=preprocess)\n",
    "laion_european_actresses = ImageFolder('./data/laion_european_celebs/actresses/images', transform=preprocess)\n",
    "print(f'European Celebs Dataset size: {len(laion_european_actors) + len(laion_european_actresses)}')\n",
    "print(f'European Celebs Total Number of Classes: {len(laion_european_actors.classes) + len(laion_european_actresses.classes)}')\n",
    "\n",
    "####################################\n",
    "# IMPORTANT: Not all of the classes in the dataset will be used for the experiments. Only classes that have more than MIN_NUM_IMAGES_AVAILABLE available are used for the experiments.\n",
    "# See below (after the predictions) on how many classes are really used in the experiments.\n",
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_datasets = ConcatDataset([facescrub_datasets, laion_european_actors, laion_european_actresses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dataset size: {len(concat_datasets)}')\n",
    "print(f'First Few Classes: {concat_datasets.classes[:10]}')\n",
    "print(f'Last Few Classes: {concat_datasets.classes[-10:]}')\n",
    "print(f'Total Number of Classes: {len(concat_datasets.classes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_class_subsets = []\n",
    "for class_idx in range(len(concat_datasets.classes)):\n",
    "    subset = SingleClassSubset(concat_datasets, class_idx)\n",
    "    dataset_class_subsets.append(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first and last preprocessed image of the first class\n",
    "plt.imshow(dataset_class_subsets[0][0][0].permute(1,2,0).numpy())\n",
    "plt.show()\n",
    "plt.imshow(dataset_class_subsets[-1][0][0].permute(1,2,0).numpy())\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Members and Non-Members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the occurences of the individuals in the laion dataset\n",
    "laion_individuals = pd.read_csv('laion400m_experiments/laion_membership_occurence_count.csv', index_col=0)\n",
    "laion_individuals['class_name'] = laion_individuals['name'].str.split(\" \").str.join(\"_\")\n",
    "# get only the individuals that are also in the dataset\n",
    "laion_individuals = laion_individuals[laion_individuals['class_name'].isin([concat_datasets.classes[x.target_class] for x in dataset_class_subsets])].reset_index(drop=True)\n",
    "laion_individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laion_non_member = laion_individuals[laion_individuals['membership'] == 'non_member'].copy().reset_index(drop=True)\n",
    "print('Non-Member')\n",
    "display(laion_non_member)\n",
    "\n",
    "laion_member = laion_individuals[laion_individuals['membership'] == 'member'].copy().reset_index(drop=True)\n",
    "print('Member')\n",
    "display(laion_member.head(4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Random First and Last Names to have more Possible Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the first names\n",
    "# list was taken from https://github.com/hadley/data-baby-names/blob/master/baby-names.csv which contains the top 1k names for the years 1880-2008 released by the US social security administration\n",
    "first_names_df = pd.read_csv('./data/common_first_names.csv')\n",
    "first_names_df = first_names_df.drop(columns=['year']).drop_duplicates(['name', 'sex'])\n",
    "first_names_df['sex'] = first_names_df['sex'].apply(lambda x: 'm' if x == 'boy' else 'f')\n",
    "# take the top 1k male and female names\n",
    "first_names_df = first_names_df.sort_values('percent', ascending=False).groupby('sex').head(1000).reset_index(drop=True).drop(columns=['percent']).rename(columns={'name': 'first_name'})\n",
    "first_names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the last names\n",
    "# list was taken from the US census burea at https://www.census.gov/topics/population/genealogy/data/2010_surnames.html and contains the top 1k surnames\n",
    "last_names_df = pd.read_csv('./data/common_last_names_US_2010.csv').dropna()[['SURNAME', 'FREQUENCY (COUNT)']]\n",
    "last_names_df = last_names_df.rename(columns={'SURNAME': 'last_name', 'FREQUENCY (COUNT)': 'count'})\n",
    "last_names_df['last_name'] = last_names_df['last_name'].str.title()\n",
    "last_names_df['count'] = last_names_df['count'].str.replace(',', '').astype(int)\n",
    "last_names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the cross product of the first and last names\n",
    "full_names_df = pd.merge(first_names_df[['first_name', 'sex']], last_names_df['last_name'], how='cross')\n",
    "\n",
    "# sample as much names from each gender equally as we need\n",
    "sampled_full_names_df = full_names_df.groupby('sex').sample(int((NUM_TOTAL_NAMES - len(concat_datasets.classes)) / 2), random_state=SEED).reset_index()\n",
    "sampled_full_names_list = sampled_full_names_df.apply(lambda x: f'{x[\"first_name\"]} {x[\"last_name\"]}', axis=1).tolist()\n",
    "print(f'Length List: {len(sampled_full_names_list)}')\n",
    "sampled_full_names_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the names from facescrub with the sampled names and shuffle them\n",
    "possible_names = [x.replace(\"_\", \" \") for x in concat_datasets.classes] + sampled_full_names_list\n",
    "print(possible_names[:10], len(possible_names))\n",
    "possible_names = random.sample(possible_names, k=len(possible_names))\n",
    "print(f'Length Possible Names: {len(possible_names)}')\n",
    "possible_names[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Model for Test Purposes on the first Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare and fill the prompt templates\n",
    "prompts = []\n",
    "for name in possible_names:\n",
    "    df_dict = {}\n",
    "    for prompt_idx, prompt in enumerate(PROMPTS):\n",
    "        df_dict['class_name'] = \"_\".join(name.split(\" \"))\n",
    "        df_dict[f'prompt_{prompt_idx}'] = prompt.format(name)\n",
    "    prompts.append(df_dict)\n",
    "prompts = pd.DataFrame(prompts)\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the context vector of the possible labels\n",
    "if not LOAD_PREDICTIONS_FROM_FILE:\n",
    "    label_context_vecs = []\n",
    "    for i in range(len(PROMPTS)):\n",
    "        context = open_clip.tokenize(prompts[f'prompt_{i}'].to_numpy())\n",
    "        label_context_vecs.append(context)\n",
    "    label_context_vecs = torch.stack(label_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the embeddings for each of the models\n",
    "if not LOAD_PREDICTIONS_FROM_FILE:\n",
    "    label_context_vecs = label_context_vecs.to(device)\n",
    "\n",
    "    text_embeddings_per_model = {}\n",
    "    for model_name, model in models.items():\n",
    "        model = model.to(device)\n",
    "        text_embeddings = get_text_embeddings(model, label_context_vecs, use_tqdm=True, context_batchsize=5_000)\n",
    "        text_embeddings_per_model[model_name] = text_embeddings\n",
    "        model = model.cpu()\n",
    "\n",
    "    label_context_vecs = label_context_vecs.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREDICTIONS_FROM_FILE:\n",
    "    test_subset_dataset = dataset_class_subsets[concat_datasets.class_to_idx[laion_member['class_name'][0]]]\n",
    "    for model_name, model in models.items():\n",
    "        model = model.to(device)\n",
    "        preds = get_preds_for_dataset(model, test_subset_dataset, label_context_vecs, num_workers=2, text_embeddings=text_embeddings_per_model[model_name])\n",
    "        unique_vals, counts = [], []\n",
    "        for x in preds:\n",
    "            x = x.unique(return_counts=True)\n",
    "            unique_vals.append(x[0])\n",
    "            counts.append(x[1])\n",
    "        model = model.cpu()\n",
    "        predictions = [int(vals[count.topk(1, sorted=True)[1]]) for vals, count in zip(unique_vals, counts)]\n",
    "        print(f'Prediction Model {model_name}: {prompts[\"class_name\"].iloc[predictions].to_list()}\\t Correct Class: {concat_datasets.classes[test_subset_dataset.target_class]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREDICTIONS_FROM_FILE:\n",
    "    test_subset_dataset = dataset_class_subsets[concat_datasets.class_to_idx[laion_non_member['class_name'][0]]]\n",
    "    for model_name, model in models.items():\n",
    "        model = model.to(device)\n",
    "        preds = get_preds_for_dataset(model, test_subset_dataset, label_context_vecs, num_workers=2, text_embeddings=text_embeddings_per_model[model_name])\n",
    "        unique_vals, counts = [], []\n",
    "        for x in preds:\n",
    "            x = x.unique(return_counts=True)\n",
    "            unique_vals.append(x[0])\n",
    "            counts.append(x[1])\n",
    "        model = model.cpu()\n",
    "        predictions = [int(vals[count.topk(1, sorted=True)[1]]) for vals, count in zip(unique_vals, counts)]\n",
    "        print(f'Prediction Model {model_name}: {prompts[\"class_name\"].iloc[predictions].to_list()}\\t Correct Class: {concat_datasets.classes[test_subset_dataset.target_class]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the CLIP model on each Actress/Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREDICTIONS_FROM_FILE:\n",
    "    filtered_subsets = []\n",
    "    for subset in dataset_class_subsets:\n",
    "        if concat_datasets.classes[subset.target_class] in laion_individuals['class_name'].tolist():\n",
    "            filtered_subsets.append(subset)\n",
    "\n",
    "    concat_dataset = PTConcatDataset([subset for subset in filtered_subsets])\n",
    "    preds_per_model = {}\n",
    "    for model_name, model in models.items():\n",
    "        model = model.to(device)\n",
    "        preds = get_preds_for_dataset(model, concat_dataset, label_context_vecs, batch_size=128, num_workers=32, text_embeddings=text_embeddings_per_model[model_name])\n",
    "        model = model.cpu()\n",
    "        assert preds.shape[1] == len(concat_dataset)\n",
    "        assert preds.shape[0] == len(PROMPTS)\n",
    "        # transpose the predictions such that we have len(PROMPTS) predictions for each sample\n",
    "        preds = preds.T\n",
    "        preds_per_model[model_name] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the large list of all predictions into prediction lists for every class\n",
    "if not LOAD_PREDICTIONS_FROM_FILE:\n",
    "    preds_per_model_per_subset = {}\n",
    "    for model_name, preds in preds_per_model.items():\n",
    "        preds_per_subset = []\n",
    "        counter = 0\n",
    "        for subset in filtered_subsets:\n",
    "            subset_preds = preds[counter:counter + len(subset)]\n",
    "            assert len(subset_preds) == len(subset)\n",
    "            preds_per_subset.append(subset_preds)\n",
    "            counter += len(subset)\n",
    "        preds_per_model_per_subset[model_name] = preds_per_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREDICTIONS_FROM_FILE:\n",
    "    preds_df_per_model = {}\n",
    "    for model_name in models.keys():\n",
    "        df_list = []\n",
    "        for group_idx, (dataset_subset, preds_subset) in enumerate(zip(filtered_subsets, preds_per_model_per_subset[model_name])):\n",
    "            class_name = concat_datasets.classes[filtered_subsets[group_idx].target_class]\n",
    "            training_data_samples_info = laion_individuals[laion_individuals['class_name'] == class_name]\n",
    "            for sample_idx, pred in enumerate(preds_subset):\n",
    "                result_dict = {\n",
    "                    'group_idx': group_idx,\n",
    "                    'class_name': class_name,\n",
    "                    'sample_idx': sample_idx,\n",
    "                    'training_sample_count': training_data_samples_info['count'].values[0],\n",
    "                    'training_sample_count_bin': training_data_samples_info['bin'].values[0]\n",
    "                }\n",
    "                for i, pred_idx in enumerate(pred):\n",
    "                    result_dict[f'name_prediction_prompt_{i}'] = prompts['class_name'].iloc[int(pred_idx)]\n",
    "                df_list.append(result_dict)\n",
    "        preds_df = pd.DataFrame(df_list)\n",
    "        preds_df_per_model[model_name] = preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREDICTIONS_FROM_FILE:\n",
    "    display(preds_df_per_model[MODEL_NAMES[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only get the rows of the members and non-members\n",
    "if not LOAD_PREDICTIONS_FROM_FILE:\n",
    "    for model_name, preds_df in preds_df_per_model.items():\n",
    "        members = pd.merge(preds_df, laion_member['class_name'], on='class_name')\n",
    "        members['actual_membership'] = 'member'\n",
    "        non_members = pd.merge(preds_df, laion_non_member['class_name'], on='class_name')\n",
    "        non_members['actual_membership'] = 'non_member'\n",
    "        preds_df_per_model[model_name] = pd.concat([members, non_members]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the predictions to file if necessary to prevent long runtimes\n",
    "if not LOAD_PREDICTIONS_FROM_FILE:\n",
    "    for model_name, preds_df in preds_df_per_model.items():\n",
    "        preds_df.to_csv(f'laion400m_experiments/prediction_dfs/predictions_multiprompt_laion400_{model_name}_{NUM_TOTAL_NAMES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.csv')\n",
    "else:\n",
    "    preds_df_per_model = {}\n",
    "    for model_name, preds_df in models.items():\n",
    "        preds_df_per_model[model_name] = pd.read_csv(f'laion400m_experiments/prediction_dfs/predictions_multiprompt_laion400_{model_name}_{NUM_TOTAL_NAMES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df_per_model[MODEL_NAMES[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter all individuals which have less than the specified number of images available\n",
    "for model_name, preds_df in preds_df_per_model.items():\n",
    "    num_samples = preds_df.groupby('class_name').transform('size')\n",
    "    preds_df_per_model[model_name] = preds_df[num_samples >= MIN_NUM_IMAGES_AVAILABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter all individuals which occure more often in the training set than the specified amount\n",
    "for model_name, preds_df in preds_df_per_model.items():\n",
    "    preds_df_per_model[model_name] = preds_df[preds_df['training_sample_count'] < MAX_NUM_TRAINING_SAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_sizes_per_model = {}\n",
    "for model_name, preds_df in preds_df_per_model.items():\n",
    "    min_num_images = preds_df.value_counts('class_name').sort_values()[0]\n",
    "    assert min_num_images == MIN_NUM_IMAGES_AVAILABLE\n",
    "    subsample_sizes = np.arange(1, min_num_images+1, 2).tolist()\n",
    "    subsample_sizes.append(30)\n",
    "    subsample_sizes_per_model[model_name] = subsample_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_membership_metrics(df, sample_size, sample_draws):\n",
    "    subsampled_dfs = []\n",
    "    subsampled_metrics_dfs = []\n",
    "    for i in range(sample_draws):\n",
    "        # sample the same number of images/predictions for each person\n",
    "        name_predictions_df = preds_df.groupby('class_name').sample(sample_size).reset_index(drop=True)\n",
    "\n",
    "        # get the number of members and non_members\n",
    "        num_member, num_non_member = name_predictions_df[['class_name', 'actual_membership']].drop_duplicates()['actual_membership'].value_counts()\n",
    "\n",
    "        # get the column names of the different prompts\n",
    "        prompt_column_names = [f'name_prediction_prompt_{i}' for i in range(len(PROMPTS))]\n",
    "\n",
    "        def get_name_predictions(predictions: pd.Series, values_only=False, counts_only=False):\n",
    "            \"\"\"Takes a series of predictions and returns the unique values and the number of prediction occurrences in descending order.\"\"\"\n",
    "            values, counts = np.unique(predictions, return_counts=True)\n",
    "            descending_counts_indices = counts.argsort()[::-1]\n",
    "\n",
    "            if values_only:\n",
    "                return values[descending_counts_indices]\n",
    "            elif counts_only:\n",
    "                return counts[descending_counts_indices]\n",
    "            else:\n",
    "                return values[descending_counts_indices], counts[descending_counts_indices]\n",
    "\n",
    "        name_prediction_count_df = name_predictions_df.groupby('class_name')[prompt_column_names].agg(list)\n",
    "        name_prediction_count_df[[f'unique_name_predictions_prompt_{i}' for i in range(len(prompt_column_names))]] = name_prediction_count_df[prompt_column_names].apply(lambda x: x.apply(lambda y: get_name_predictions(y, values_only=True)))\n",
    "        name_prediction_count_df[[f'unique_name_prediction_count_prompt_{i}' for i in range(len(prompt_column_names))]] = name_prediction_count_df[prompt_column_names].apply(lambda x: x.apply(lambda y: get_name_predictions(y, counts_only=True)))\n",
    "\n",
    "        # get the actual membership by merging with the sampled dataframe\n",
    "        name_prediction_count_df = pd.merge(name_prediction_count_df, name_predictions_df[['class_name', 'actual_membership']].drop_duplicates().set_index('class_name'), how='inner', on='class_name')\n",
    "\n",
    "        def check_for_correct_prompt_majority(row: pd.Series):\n",
    "            \"\"\"Takes a row of the dataframe and checks whether the correct name was predicted the majority of the time.\"\"\"\n",
    "            # iterate the prompts\n",
    "            num_correct_prompts = 0\n",
    "            for prompt_idx in range(len(row[prompt_column_names])):\n",
    "                unique_predictions = row[f'unique_name_predictions_prompt_{prompt_idx}']\n",
    "                prediction_counts = row[f'unique_name_prediction_count_prompt_{prompt_idx}']\n",
    "                \n",
    "                # get the indices of the most often predicted names\n",
    "                idx_most_often_pred_names = np.argwhere(prediction_counts == prediction_counts.max()).flatten()\n",
    "\n",
    "                # if there are two or more names predicted the same time, we don't have a clear majority prediction and therefore skip this prompt\n",
    "                if len(idx_most_often_pred_names) > 1:\n",
    "                    continue\n",
    "\n",
    "                # if a name was predicted by the majority and it is the correct name, we have a correct majority prediction\n",
    "                if unique_predictions[idx_most_often_pred_names[0]] == row.name:\n",
    "                    assert len(idx_most_often_pred_names) == 1\n",
    "                    num_correct_prompts += 1\n",
    "\n",
    "            # return true if the number of prompts is greater or equal to the threshold\n",
    "            return num_correct_prompts >= MIN_NUM_CORRECT_PROMPT_PREDS\n",
    "\n",
    "        name_prediction_count_df['correct_majority_prediction'] = name_prediction_count_df.apply(check_for_correct_prompt_majority, axis=1)\n",
    "        name_prediction_count_df['membership_prediction'] = name_prediction_count_df['correct_majority_prediction'].apply(lambda x: 'member' if x else 'non_member')\n",
    "        name_prediction_count_df['sample_size'] = sample_size\n",
    "        name_prediction_count_df['draw'] = i\n",
    "\n",
    "        tp = len(name_prediction_count_df[(name_prediction_count_df['membership_prediction'] == 'member') & (name_prediction_count_df['actual_membership'] == 'member')])\n",
    "        fp = len(name_prediction_count_df[(name_prediction_count_df['membership_prediction'] == 'member') & (name_prediction_count_df['actual_membership'] == 'non_member')])\n",
    "        fn = len(name_prediction_count_df[(name_prediction_count_df['membership_prediction'] == 'non_member') & (name_prediction_count_df['actual_membership'] == 'member')])\n",
    "        tn = len(name_prediction_count_df[(name_prediction_count_df['membership_prediction'] == 'non_member') & (name_prediction_count_df['actual_membership'] == 'non_member')])\n",
    "\n",
    "        subsampled_metrics_dfs.append({\n",
    "            'sample_size': sample_size,\n",
    "            'draw': i,\n",
    "            'tpr': tp / num_member,\n",
    "            'fnr': fn / num_member,\n",
    "            'fpr': fp / num_non_member,\n",
    "            'tnr': tn / num_non_member,\n",
    "            'tp': tp,\n",
    "            'fn': fn,\n",
    "            'fp': fp,\n",
    "            'tn': tn\n",
    "        })\n",
    "        subsampled_dfs.append(name_prediction_count_df)\n",
    "    \n",
    "    return subsampled_metrics_dfs, subsampled_dfs\n",
    "\n",
    "\n",
    "class TQDMParallel(Parallel):\n",
    "    def __init__(self, progress_bar=True, total=None, *args, **kwargs):\n",
    "        self.progress_bar = progress_bar\n",
    "        self.total = total\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        with tqdm(disable=not self.progress_bar, total=self.total) as self.pbar:\n",
    "            return Parallel.__call__(self, *args, **kwargs)\n",
    "\n",
    "    def print_progress(self):\n",
    "        if self.total is None:\n",
    "            self.pbar.total = self.n_dispatched_tasks\n",
    "        self.pbar.n = self.n_completed_tasks\n",
    "        self.pbar.refresh()\n",
    "\n",
    "if not LOAD_PREDICTION_METRICS_FROM_FILE:\n",
    "    subsampled_metrics_dfs_per_model = {}\n",
    "    subsampled_dfs_per_model = {}\n",
    "    sample_draws = 20\n",
    "    for model_name, preds_df in preds_df_per_model.items():\n",
    "        subsample_sizes = subsample_sizes_per_model[model_name]\n",
    "        arguments_list = []\n",
    "        for sample_size in subsample_sizes:\n",
    "            arguments_list.append((preds_df, sample_size, sample_draws))\n",
    "\n",
    "        print(f'{model_name} with {len(arguments_list)} predictions')\n",
    "        results = TQDMParallel(n_jobs=16, total=len(arguments_list))(\n",
    "            delayed(get_membership_metrics)(*arguments) for arguments in arguments_list\n",
    "        )\n",
    "\n",
    "        flattened_subsampled_metrics_dfs = []\n",
    "        flattened_subsampled_dfs = []\n",
    "        for res in results:\n",
    "            [flattened_subsampled_metrics_dfs.append(x) for x in res[0]]\n",
    "            [flattened_subsampled_dfs.append(x) for x in res[1]]\n",
    "        subsampled_metrics_dfs_per_model[model_name] = flattened_subsampled_metrics_dfs\n",
    "        subsampled_dfs_per_model[model_name] = flattened_subsampled_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREDICTION_METRICS_FROM_FILE:\n",
    "    for model_name, subsampled_metrics_dfs in subsampled_metrics_dfs_per_model.items():\n",
    "        subsampled_metrics_dfs_per_model[model_name] = pd.DataFrame(subsampled_metrics_dfs).set_index('sample_size').drop('draw', axis='columns')\n",
    "        subsampled_metrics_dfs_per_model[model_name] = subsampled_metrics_dfs_per_model[model_name].rename(columns={'tpr': 'True Positive Rate', 'fnr': 'False Negative Rate', 'fpr': 'False Positive Rate', 'tnr': 'True Negative Rate'})\n",
    "        subsampled_metrics_dfs_per_model[model_name].index.name = 'Number of Samples'\n",
    "\n",
    "    for model_name, subsampled_dfs in subsampled_dfs_per_model.items():\n",
    "        subsampled_dfs_per_model[model_name] = pd.concat(subsampled_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREDICTION_METRICS_FROM_FILE:\n",
    "    for model_name, _ in subsampled_metrics_dfs_per_model.items():\n",
    "        subsampled_metrics_dfs_per_model[model_name].to_csv(f'./laion400m_experiments/prediction_metrics_dfs/multiprompt_prediction_metrics_laion400_{model_name}_{NUM_TOTAL_NAMES}_{MAX_NUM_TRAINING_SAMPLES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.csv')\n",
    "        subsampled_dfs_per_model[model_name].reset_index().to_feather(f'./laion400m_experiments/prediction_metrics_dfs/multiprompt_prediction_subsamples_laion400_{model_name}_{NUM_TOTAL_NAMES}_{MAX_NUM_TRAINING_SAMPLES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.feather')\n",
    "else:\n",
    "    subsampled_metrics_dfs_per_model = {}\n",
    "    subsampled_dfs_per_model = {}\n",
    "    for model_name, model in models.items():\n",
    "        subsampled_metrics_dfs_per_model[model_name] = pd.read_csv(f'./laion400m_experiments/prediction_metrics_dfs/multiprompt_prediction_metrics_laion400_{model_name}_{NUM_TOTAL_NAMES}_{MAX_NUM_TRAINING_SAMPLES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.csv', index_col=0)\n",
    "        subsampled_dfs_per_model[model_name] = pd.read_feather(f'./laion400m_experiments/prediction_metrics_dfs/multiprompt_prediction_subsamples_laion400_{model_name}_{NUM_TOTAL_NAMES}_{MAX_NUM_TRAINING_SAMPLES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(subsampled_metrics_dfs_per_model[MODEL_NAMES[0]])\n",
    "display(subsampled_dfs_per_model[MODEL_NAMES[-1]].reset_index())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, df in subsampled_metrics_dfs_per_model.items():\n",
    "    display(df.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the accuracy for each sampling\n",
    "for model_name, subsampled_dfs in subsampled_metrics_dfs_per_model.items():\n",
    "    subsampled_metrics_dfs_per_model[model_name]['Accuracy'] = (subsampled_dfs['tp'] + subsampled_dfs['tn']) / (subsampled_dfs['tp'] + subsampled_dfs['tn'] + subsampled_dfs['fp'] + subsampled_dfs['fn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only use 3 or more images since we need a majority vote\n",
    "for model_name, _ in subsampled_metrics_dfs_per_model.items():\n",
    "    subsampled_metrics_dfs_per_model[model_name] = subsampled_metrics_dfs_per_model[model_name][subsampled_metrics_dfs_per_model[model_name].index >= 3]\n",
    "    subsampled_dfs_per_model[model_name] = subsampled_dfs_per_model[model_name][subsampled_dfs_per_model[model_name]['sample_size'] >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "show_y_axis_only_for_first_model = True\n",
    "show_legend_only_in_first_plot = True\n",
    "show_legend = False\n",
    "for model_name, subsampled_dfs in subsampled_metrics_dfs_per_model.items():\n",
    "    plt.clf()\n",
    "    data = subsampled_dfs[[ 'Accuracy', 'True Positive Rate', 'False Negative Rate', 'False Positive Rate', 'True Negative Rate']]\n",
    "    data = data.rename(columns={\n",
    "        'Accuracy': 'Acc',\n",
    "        'True Positive Rate': 'TPR', \n",
    "        'False Negative Rate': 'FNR', \n",
    "        'False Positive Rate': 'FPR', \n",
    "        'True Negative Rate': 'TNR'\n",
    "        }\n",
    "    )\n",
    "    display(data.groupby('Number of Samples').mean())\n",
    "    ax = sns.lineplot(data=data, errorbar='sd', palette='colorblind')\n",
    "\n",
    "    ax.set_xlabel(\"Number of Images used for IDIA\", weight=\"bold\", size=16)\n",
    "    x_ticks = [i for i in range(0, data.index.unique().max()+1, 5)]\n",
    "    x_ticks[0] = 3\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels([int(x) for x in ax.get_xticks()], size=16)\n",
    "\n",
    "    ax.set_yticks([i for i in np.arange(0, 1+0.1, 0.1)])\n",
    "    ax.set_yticklabels(ax.get_yticks(), size=16)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "    \n",
    "    # show only every second y tick label\n",
    "    plt.setp(ax.yaxis.get_ticklabels()[1::2], visible=False)\n",
    "\n",
    "    if show_legend or (show_legend_only_in_first_plot and model_name == MODEL_NAMES[0]):\n",
    "        h, l = ax.get_legend_handles_labels()\n",
    "        ax.legend(h, l, ncol=3, loc='upper center', fontsize=16, bbox_to_anchor=(0, -0.05, 1, 1))\n",
    "    else:\n",
    "        ax.legend_.remove()\n",
    "\n",
    "    if show_y_axis_only_for_first_model and model_name != MODEL_NAMES[0]:\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if not os.path.exists('./laion400m_experiments/plots'):\n",
    "        os.mkdir('./laion400m_experiments/plots')\n",
    "    ax.get_figure().savefig(f'./laion400m_experiments/plots/num_idia_samples_plot_multiprompt_laion400_{model_name}_{NUM_TOTAL_NAMES}_{MAX_NUM_TRAINING_SAMPLES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.pdf')\n",
    "    ax.get_figure().savefig(f'./laion400m_experiments/plots/num_idia_samples_plot_multiprompt_laion400_{model_name}_{NUM_TOTAL_NAMES}_{MAX_NUM_TRAINING_SAMPLES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.png', dpi=100)\n",
    "    print(model_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_y_axis_only_for_first_model = True\n",
    "for model_name, subsampled_dfs in subsampled_metrics_dfs_per_model.items():\n",
    "    tp_std, fn_std, fp_std, tn_std = subsampled_dfs.groupby('Number of Samples').std().iloc[-1][['tp', 'fn', 'fp', 'tn']]\n",
    "    tp, fn, fp, tn = subsampled_dfs.groupby('Number of Samples').mean().iloc[-1][['tp', 'fn', 'fp', 'tn']]\n",
    "\n",
    "    tpr_std, fnr_std, fpr_std, tnr_std = subsampled_dfs.groupby('Number of Samples').std().iloc[-1][['True Positive Rate', 'False Negative Rate', 'False Positive Rate', 'True Negative Rate']]\n",
    "    tpr, fnr, fpr, tnr = subsampled_dfs.groupby('Number of Samples').mean().iloc[-1][['True Positive Rate', 'False Negative Rate', 'False Positive Rate', 'True Negative Rate']]\n",
    "\n",
    "    normalized_conf_mat = pd.DataFrame({'Member': [tpr, fpr], 'Non-Member': [fnr, tnr]}, index=['Member', 'Non-Member'])\n",
    "    normalized_conf_mat.index.set_names('Actual Membership', inplace=True)\n",
    "    normalized_conf_mat = normalized_conf_mat.rename_axis('Predicted Membership', axis='columns')\n",
    "\n",
    "    group_names = ['TP','FN','FP','TN']\n",
    "    group_counts = [\"{0:0.0f} \\u00B1 {1:0.2f}\".format(mean, std) for mean, std in zip([tp, fn, fp, tn], [tp_std, fn_std, fp_std, tn_std])]\n",
    "    percentage = [\"{0:0.2f}% \\u00B1 {1:0.02f}%\".format(mean * 100, std * 100) for mean, std in zip([tpr, fnr, fpr, tnr], [tpr_std, fnr_std, fpr_std, tnr_std])]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names, group_counts, percentage)]\n",
    "    plt.clf()\n",
    "    ax = sns.heatmap(normalized_conf_mat, annot=np.asarray(labels).reshape(2, 2), fmt='', cbar=False, cmap='Blues', annot_kws={'fontsize': 16})\n",
    "\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), size=16)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), size=16)\n",
    "\n",
    "    if show_y_axis_only_for_first_model and model_name != MODEL_NAMES[0]:\n",
    "        ax.set_yticklabels([])\n",
    "        plt.ylabel('')\n",
    "    else:\n",
    "        plt.ylabel('Actual Membership', fontsize=16, weight='bold')\n",
    "\n",
    "    plt.xlabel('Predicted Membership', fontsize=16, weight='bold')\n",
    "    plt.tight_layout()\n",
    "    ax.get_figure().savefig(f'./laion400m_experiments/plots/confusion_matrix_multiprompt_laion400_{model_name}_{NUM_TOTAL_NAMES}_{MAX_NUM_TRAINING_SAMPLES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.pdf')\n",
    "    ax.get_figure().savefig(f'./laion400m_experiments/plots/confusion_matrix_multiprompt_laion400_{model_name}_{NUM_TOTAL_NAMES}_{MAX_NUM_TRAINING_SAMPLES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.png', dpi=100)\n",
    "    print(model_name)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Metrics Against the Number of Samples in the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "show_legend_only_for = MODEL_NAMES[-1]\n",
    "show_y_axis_only_for = MODEL_NAMES[-1]\n",
    "for model_name in models.keys():\n",
    "    plt.clf()\n",
    "    data = subsampled_dfs_per_model[model_name]\n",
    "    # get only the subsamples where 30 samples were used\n",
    "    data = data[data['sample_size'] == data.groupby('sample_size').last().iloc[-1].name].copy(deep=True)\n",
    "    data = pd.merge(laion_individuals[['class_name', 'count', 'bin']], data, on='class_name')\n",
    "\n",
    "    # group by number of training sample bins and calculate the metrics\n",
    "    group_metrics = []\n",
    "    num_samples_per_group = []\n",
    "    for grp_name, group in data.groupby(['draw', 'bin']):\n",
    "        # we want to skip the non_member here\n",
    "        if grp_name[1] == '[0, 1)':\n",
    "            continue\n",
    "\n",
    "        # get the number of members and non_members\n",
    "        num_member = group[['class_name', 'actual_membership']].drop_duplicates()['actual_membership'].value_counts()[0]\n",
    "\n",
    "        tp = len(group[(group['membership_prediction'] == 'member') & (group['actual_membership'] == 'member')])\n",
    "        fn = len(group[(group['membership_prediction'] == 'non_member') & (group['actual_membership'] == 'member')])\n",
    "\n",
    "        group_metrics.append({\n",
    "            'draw': grp_name[0],\n",
    "            'bin': pd.Interval(*map(int, re.sub(r\"(\\[|\\))\", \"\", grp_name[1]).split(\", \")), closed='left'),\n",
    "            'True Positive Rate': tp / num_member,\n",
    "            'False Negative Rate': fn / num_member\n",
    "        })\n",
    "        num_samples_per_group.append({\n",
    "            'num_samples': len(group),\n",
    "            'bin': pd.Interval(*map(int, re.sub(r\"(\\[|\\))\", \"\", grp_name[1]).split(\", \")), closed='left'),\n",
    "            'draw': grp_name[0]\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(group_metrics).rename(columns={'bin': 'Number of Samples'}).set_index('Number of Samples')\n",
    "    num_samples_df =  pd.DataFrame(num_samples_per_group)[['num_samples', 'bin']].drop_duplicates().rename(columns={'bin': 'Number of Samples'}).set_index('Number of Samples')\n",
    "    \n",
    "    interval_index_mapping = {x: i for i, x in enumerate(df.index.sort_values().unique())}\n",
    "    df['idx'] = df.apply(lambda x: interval_index_mapping[x.name], axis=1)\n",
    "    num_samples_df['idx'] = num_samples_df.apply(lambda x: interval_index_mapping[x.name], axis=1)\n",
    "\n",
    "\n",
    "    # plot the graph\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    rows_to_plot = ['True Positive Rate', 'False Negative Rate']\n",
    "    print(model_name)\n",
    "    display(df[rows_to_plot].groupby(df.index).mean())\n",
    "\n",
    "    ax = sns.lineplot(data=df[['idx'] + rows_to_plot].rename(columns={'idx': 'Number of Training Samples per Person'}).set_index('Number of Training Samples per Person'), errorbar='sd', ax=ax)\n",
    "    \n",
    "    ax.set_xticks(df['idx'].sort_values().unique())\n",
    "    ax.set_xticklabels(df.index.sort_values().unique().astype(str).tolist(), rotation=45, ha=\"right\", rotation_mode=\"anchor\", size=16)\n",
    "    #ax.set_xticklabels([x.right for x in df.index.sort_values().unique().tolist()], size=16)\n",
    "    ax.set_xlabel(\"Number of Images\\nper Person in LAION-400M\", weight=\"bold\", size=16)\n",
    "\n",
    "    ax.set_yticks([i for i in np.arange(0, 1+0.1, 0.1)])\n",
    "    ax.set_yticklabels(ax.get_yticks(), size=16)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "    ax.set_ylim(-0.025, 1.025)\n",
    "\n",
    "    # show only every second y tick label\n",
    "    plt.setp(ax.yaxis.get_ticklabels()[1::2], visible=False)\n",
    "\n",
    "    if model_name == show_legend_only_for:\n",
    "        h, l = ax.get_legend_handles_labels()\n",
    "        ax.legend(h, l, ncol=1, fontsize=16)\n",
    "    else:\n",
    "        ax.legend_.remove()\n",
    "\n",
    "    if model_name != show_y_axis_only_for:\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    ax.get_figure().savefig(f'./laion400m_experiments/plots/num_training_samples_plot_multiprompt_laion400_{model_name}_{NUM_TOTAL_NAMES}_{MAX_NUM_TRAINING_SAMPLES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.pdf', bbox_inches='tight')\n",
    "    ax.get_figure().savefig(f'./laion400m_experiments/plots/num_training_samples_plot_multiprompt_laion400_{model_name}_{NUM_TOTAL_NAMES}_{MAX_NUM_TRAINING_SAMPLES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.png', dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "display(num_samples_df)\n",
    "print(f'Total number of individuals used: {num_samples_df[\"num_samples\"].sum()}')\n",
    "\n",
    "plt.clf()\n",
    "ax = sns.barplot(num_samples_df[['num_samples']].rename(columns={'num_samples': 'Number of Individuals'}).sort_values('Number of Samples').reset_index(), y='Number of Individuals', x='Number of Samples')\n",
    "ax.set_xticklabels(num_samples_df.index.sort_values().unique().astype(str).tolist(), size=16, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "ax.set_yticklabels(ax.get_yticklabels(), size=16)\n",
    "ax.set_xlabel(\"Number of Images\\nper Individual in LAION-400M\", size=16, weight=\"bold\")\n",
    "ax.set_ylabel(\"Number of\\nIndividuals\", size=16, weight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "ax.get_figure().savefig(f'./laion400m_experiments/plots/num_training_samples_histogram_laion400_{NUM_TOTAL_NAMES}_{MAX_NUM_TRAINING_SAMPLES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.pdf')\n",
    "ax.get_figure().savefig(f'./laion400m_experiments/plots/num_training_samples_histogram_laion400_{NUM_TOTAL_NAMES}_{MAX_NUM_TRAINING_SAMPLES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.png', dpi=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot a Heatmap to Visualize Influence of Number of Attack/Trainin Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cbar = False\n",
    "show_ylabel = True\n",
    "for model_name in models.keys():\n",
    "    plt.clf()\n",
    "    data = subsampled_dfs_per_model[model_name]\n",
    "    data = pd.merge(laion_individuals[['class_name', 'count', 'bin']], data, on='class_name')\n",
    "\n",
    "    dfs = []\n",
    "    for sample_size, sample_size_group in data.groupby('sample_size'):\n",
    "        # group by number of training sample bins and calculate the metrics\n",
    "        group_metrics = []\n",
    "        num_samples_per_group = []\n",
    "        for grp_name, group in sample_size_group.groupby(['draw', 'bin']):\n",
    "            # we want to skip the non_member here\n",
    "            if grp_name[1] == '[0, 1)':\n",
    "                continue\n",
    "\n",
    "            # get the number of members and non_members\n",
    "            num_member = group[['class_name', 'actual_membership']].drop_duplicates()['actual_membership'].value_counts()[0]\n",
    "\n",
    "            tp = len(group[(group['membership_prediction'] == 'member') & (group['actual_membership'] == 'member')])\n",
    "            fn = len(group[(group['membership_prediction'] == 'non_member') & (group['actual_membership'] == 'member')])\n",
    "\n",
    "            group_metrics.append({\n",
    "                'draw': grp_name[0],\n",
    "                'bin': pd.Interval(*map(int, re.sub(r\"(\\[|\\))\", \"\", grp_name[1]).split(\", \")), closed='left'),\n",
    "                'True Positive Rate': tp / num_member,\n",
    "                'False Negative Rate': fn / num_member,\n",
    "                'sample_size': sample_size\n",
    "            })\n",
    "            \n",
    "        num_training_samples_label = \"Number of Images\\nper Person in LAION-400M\"\n",
    "        dfs.append(pd.DataFrame(group_metrics).rename(columns={'bin': num_training_samples_label}).set_index(num_training_samples_label).groupby(num_training_samples_label).mean())\n",
    "\n",
    "    combined_df = pd.concat(dfs)\n",
    "    pivoted_df = combined_df.reset_index().rename(columns={'sample_size': 'Number of Images\\nused for IDIA'}).pivot(index='Number of Images\\nused for IDIA', columns=num_training_samples_label, values=\"True Positive Rate\")\n",
    "    pivoted_df.index = pivoted_df.index.astype(int)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    ax = sns.heatmap(pivoted_df, yticklabels=2, vmin=0, vmax=1, cmap=\"Blues\", cbar=model_name == MODEL_NAMES[-1], ax=ax)\n",
    "\n",
    "    if model_name == MODEL_NAMES[-1]:\n",
    "        ax.figure.axes[-1].set_ylabel(\"True Positive Rate\", weight='bold', size=16)\n",
    "        ax.collections[0].colorbar.ax.tick_params(labelsize=16)\n",
    "\n",
    "\n",
    "    ax.set_xlabel(ax.get_xlabel(), weight=\"bold\", size=16)\n",
    "    ax.set_ylabel(ax.get_ylabel(), weight=\"bold\", size=16)\n",
    "\n",
    "    ax.set_xticklabels([x.get_text() for x in ax.get_xticklabels()], size=16, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 16)\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    if model_name != MODEL_NAMES[0]:\n",
    "        ax.set(yticklabels=[\" \" for x in ax.get_yticklabels()], ylabel=\" \")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    ax.get_figure().savefig(f'./laion400m_experiments/plots/heatmap_num_training_samples_laion400_{model_name}_{NUM_TOTAL_NAMES}_{MAX_NUM_TRAINING_SAMPLES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.pdf', bbox_inches='tight')\n",
    "    ax.get_figure().savefig(f'./laion400m_experiments/plots/heatmap_num_training_samples_laion400_{model_name}_{NUM_TOTAL_NAMES}_{MAX_NUM_TRAINING_SAMPLES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.png', dpi=100, bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
