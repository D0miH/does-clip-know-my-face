{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "from IPython.display import display\n",
    "from clip_retrieval.clip_client import ClipClient, Modality\n",
    "from IPython.display import Image, display\n",
    "from base64 import b64decode\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import open_clip\n",
    "from PIL import Image as PILImage\n",
    "import os\n",
    "import wandb\n",
    "import cv2\n",
    "import itertools\n",
    "from pl_bolts.transforms.dataset_normalizations import imagenet_normalization\n",
    "import torchvision.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "os.chdir('/workspace')\n",
    "from rtpt.rtpt import setproctitle\n",
    "setproctitle('@Clipping_Privacy_CC3M_Notebook')\n",
    "\n",
    "from facescrub_training.pl_models.resnet import ResNet50\n",
    "from facescrub_training.datasets import FaceScrubCropped\n",
    "from utils import TQDMParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the backend of clip-retrieval has to be started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Clip Retrieval Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_BACKEND = False\n",
    "NUM_IMAGES = 50\n",
    "# after starting the docker containers get the ip addresses using the followinig command:\n",
    "# docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' <docker_container_name>\"\n",
    "CLIENT_URLS = [\n",
    "    'http://172.17.0.3:1337/knn-service'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = []\n",
    "for url in CLIENT_URLS:\n",
    "    clients.append(\n",
    "        ClipClient(\n",
    "            url=url,\n",
    "            indice_name='CC3M-Train',\n",
    "            aesthetic_weight=0,\n",
    "            modality=Modality.IMAGE,\n",
    "            use_safety_model=False,\n",
    "            use_violence_detector=False,\n",
    "            deduplicate=False,\n",
    "            num_images=NUM_IMAGES\n",
    "        )\n",
    "    )\n",
    "len(clients)\n",
    "\n",
    "def log_result(result):\n",
    "    image_path, image, id, similarity = result['image_path'], result['image'], result['id'], result['similarity']\n",
    "    print(f\"id: {id}\")\n",
    "    print(f\"similarity: {similarity}\")\n",
    "    display(Image(b64decode(image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Clip Retrieval to make sure it works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if QUERY_BACKEND:\n",
    "    cat = clients[-1].query(text='an image of a cat')\n",
    "    print(len(cat))\n",
    "    log_result(cat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the FaceScrub Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_dataset = ImageFolder(root='./data/facescrub/actors/images')\n",
    "actresses_dataset = ImageFolder(root='./data/facescrub/actresses/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(actors_dataset[0][0])\n",
    "plt.show()\n",
    "plt.imshow(actresses_dataset[0][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the NUM_IMAGES most similar images to each of the images in the FaceScrub dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_images_from_dataset(dataset, clip_retrieval_client):\n",
    "    similar_images = {}\n",
    "    for img, cls in tqdm(dataset.imgs, desc='Getting Similar Images', total=len(dataset)):\n",
    "        res = []\n",
    "        try:\n",
    "            res = clip_retrieval_client.query(image=img)\n",
    "            [x.pop('image', None) for x in res]\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "        \n",
    "        similar_images[img] = res\n",
    "\n",
    "    return similar_images   \n",
    "\n",
    "def get_similar_images(img, clip_retrieval_client):\n",
    "    res = []\n",
    "    try:\n",
    "        res = clip_retrieval_client.query(image=img)\n",
    "        [x.pop('image', None) for x in res]\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if QUERY_BACKEND:\n",
    "    # get the similar images as list\n",
    "    sim_imgs_actors = TQDMParallel(n_jobs=len(clients), total=len(actors_dataset))(delayed(get_similar_images)(actors_dataset.imgs[i][0], clients[i%len(clients)]) for i in range(len(actors_dataset)))\n",
    "\n",
    "    # convert the list to a dictionary\n",
    "    actors_sim_imgs = {}\n",
    "    for i, (img, cls)in enumerate(actors_dataset.imgs):\n",
    "        actors_sim_imgs[img] = sim_imgs_actors[i]\n",
    "\n",
    "    # save the dictionary as a json file\n",
    "    with open(f'cc3m_experiments/face_scrub_top{NUM_IMAGES}_similar_conceptual_caption_images_actors.json', 'w') as json_file:\n",
    "        json_file.write(json.dumps(actors_sim_imgs))\n",
    "else:\n",
    "    with open('cc3m_experiments/face_scrub_top{NUM_IMAGES}_similar_conceptual_caption_images_actors.json', 'r') as json_file:\n",
    "        actors_sim_imgs = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if QUERY_BACKEND:\n",
    "    # do the same as above for the actresses\n",
    "    # get the similar images as list\n",
    "    sim_imgs_actresses = TQDMParallel(n_jobs=len(clients), total=len(actresses_dataset))(delayed(get_similar_images)(actresses_dataset.imgs[i][0], clients[i%len(clients)]) for i in range(len(actresses_dataset)))\n",
    "\n",
    "    # convert the list to a dictionary\n",
    "    actresses_sim_imgs = {}\n",
    "    for i, (img, cls)in enumerate(actresses_dataset.imgs):\n",
    "        actresses_sim_imgs[img] = sim_imgs_actresses[i]\n",
    "\n",
    "    # save the dictionary as a json file\n",
    "    with open('cc3m_experiments/face_scrub_top{NUM_IMAGES}_similar_conceptual_caption_images_actresses.json', 'w') as json_file:\n",
    "        json_file.write(json.dumps(actresses_sim_imgs))\n",
    "else:\n",
    "    with open('cc3m_experiments/face_scrub_top{NUM_IMAGES}_similar_conceptual_caption_images_actresses.json', 'r') as json_file:\n",
    "        actresses_sim_imgs = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(sim_imgs):\n",
    "    dataframes = []\n",
    "    for key in sim_imgs.keys():\n",
    "        df = pd.DataFrame(sim_imgs[key])\n",
    "        df['image'] = key\n",
    "        dataframes.append(df)\n",
    "    \n",
    "    df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    df = df[list(df.columns)[::-1]]\n",
    "    df['name'] = df.image.str.split('/').str[-1].str.split('.').str[:-1].str.join('.').str.split('_').str[:-1].str.join('_')\n",
    "\n",
    "    return df\n",
    "\n",
    "actresses_df = create_df(actresses_sim_imgs)\n",
    "actors_df = create_df(actors_sim_imgs)\n",
    "actors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the Persons on the Images using the OpenClip Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageListDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_list, transform=None):\n",
    "        super().__init__()\n",
    "        self.img_list = image_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_pth = self.img_list[idx]\n",
    "        img = PILImage.open(img_pth)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-32-quickgelu\", pretrained='laion400m_e32', device=device)\n",
    "actors_split_class_names = [x.replace(\"_\", \" \") for x in actors_dataset.classes]\n",
    "actors_label_context_vecs = open_clip.tokenize(actors_split_class_names).to(device)\n",
    "actresses_split_class_names = [x.replace(\"_\", \" \") for x in actresses_dataset.classes]\n",
    "actresses_label_context_vecs = open_clip.tokenize(actresses_split_class_names).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_classes_actors = pd.DataFrame(actors_dataset.class_to_idx.items(), columns=['class', 'idx']).set_index('idx')\n",
    "index_to_classes_actresses = pd.DataFrame(actresses_dataset.class_to_idx.items(), columns=['class', 'idx']).set_index('idx')\n",
    "display(index_to_classes_actors)\n",
    "display(index_to_classes_actresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageListDataset(actors_df['image_path'], transform=preprocess)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, num_workers=8, pin_memory=device=='cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = []\n",
    "    for x in tqdm(dataloader):\n",
    "        x = x.to(device)\n",
    "        image_features, text_features, logits_scale = clip(x, actors_label_context_vecs)\n",
    "        # we have to calculate the cosine similarity manually. OpenAI does this internally.\n",
    "        logits_per_image = logits_scale  * image_features @ text_features.T\n",
    "        preds.append(logits_per_image.argmax(-1).cpu())\n",
    "\n",
    "    preds = torch.cat(preds)\n",
    "\n",
    "    actors_df['openclip_prediction'] = index_to_classes_actors.reindex(preds.tolist())['class'].tolist()\n",
    "actors_df.to_csv('cc3m_experiments/facescrub_top50_images_predictions_VitB32_OpenCLIP_actors.csv')\n",
    "actors_df = pd.read_csv('cc3m_experiments/facescrub_top50_images_predictions_VitB32_OpenCLIP_actors.csv', index_col=0)\n",
    "actors_df['gender'] = 'm'\n",
    "actors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageListDataset(actresses_df['image_path'], transform=preprocess)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, num_workers=8, pin_memory=device=='cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = []\n",
    "    for x in tqdm(dataloader):\n",
    "        x = x.to(device)\n",
    "        image_features, text_features, logits_scale = clip(x, actresses_label_context_vecs)\n",
    "        # we have to calculate the cosine similarity manually. OpenAI does this internally.\n",
    "        logits_per_image = logits_scale  * image_features @ text_features.T\n",
    "        preds.append(logits_per_image.argmax(-1).cpu())\n",
    "\n",
    "    preds = torch.cat(preds)\n",
    "\n",
    "    actresses_df['openclip_prediction'] = index_to_classes_actresses.reindex(preds.tolist())['class'].tolist()\n",
    "actresses_df.to_csv('cc3m_experiments/facescrub_top50_images_predictions_VitB32_OpenCLIP_actresses.csv')\n",
    "actresses_df = pd.read_csv('cc3m_experiments/facescrub_top50_images_predictions_VitB32_OpenCLIP_actresses.csv', index_col=0)\n",
    "actresses_df['gender'] = 'f'\n",
    "actresses_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Faces in the similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_bb(image_pth_list):\n",
    "    face_cascade = cv2.CascadeClassifier(os.path.join(cv2.data.haarcascades, 'haarcascade_frontalface_default.xml'))\n",
    "\n",
    "    bbs = []\n",
    "    for image_path in image_pth_list:\n",
    "        img = cv2.imread(image_path)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(32, 32))\n",
    "        if len(faces) > 0:\n",
    "            faces = faces.tolist()\n",
    "        else:\n",
    "            faces = []\n",
    "\n",
    "        bbs.append(faces)\n",
    "    \n",
    "    return bbs\n",
    "\n",
    "num_workers = 64\n",
    "chunk_size = 500\n",
    "image_path_list_chunks = [actors_df['image_path'][i:i+chunk_size].tolist() for i in range(0, len(actors_df['image_path']), chunk_size)]\n",
    "face_bbs = TQDMParallel(\n",
    "    n_jobs=num_workers, total=len(image_path_list_chunks)\n",
    ")(\n",
    "    delayed(get_face_bb)(chunk) for chunk in image_path_list_chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_bbs = list(itertools.chain(*face_bbs))\n",
    "actors_df['face_bbs'] = face_bbs\n",
    "actors_df.to_csv('cc3m_experiments/facescrub_top50_images_predictions_VitB32_OpenCLIP_actors_with_BB.csv')\n",
    "actors_df = pd.read_csv('cc3m_experiments/facescrub_top50_images_predictions_VitB32_OpenCLIP_actors_with_BB.csv', index_col=0)\n",
    "# convert the string arrays to numpy arrays\n",
    "actors_df['face_bbs'] = actors_df['face_bbs'].apply(lambda x: np.asarray(np.matrix(x)).reshape(-1, 4))\n",
    "actors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_list_chunks = [actresses_df['image_path'][i:i+chunk_size].tolist() for i in range(0, len(actresses_df['image_path']), chunk_size)]\n",
    "face_bbs = TQDMParallel(\n",
    "    n_jobs=num_workers, total=len(image_path_list_chunks)\n",
    ")(\n",
    "    delayed(get_face_bb)(chunk) for chunk in image_path_list_chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_bbs = list(itertools.chain(*face_bbs))\n",
    "actresses_df['face_bbs'] = face_bbs\n",
    "actresses_df.to_csv('cc3m_experiments/facescrub_top50_images_predictions_VitB32_OpenCLIP_actresses_with_BB.csv')\n",
    "actresses_df = pd.read_csv('cc3m_experiments/facescrub_top50_images_predictions_VitB32_OpenCLIP_actresses_with_BB.csv', index_col=0)\n",
    "actresses_df['face_bbs'] = actresses_df['face_bbs'].apply(lambda x: np.asarray(np.matrix(x)).reshape(-1, 4))\n",
    "actresses_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the Persons on the Images using the ResNet50 trained on FaceScrub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = ResNet50.load_from_checkpoint('facescrub_training/pretrained_models/rn50_facescrub.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = FaceScrubCropped(\n",
    "    False, \n",
    "    transform=T.Compose([T.Resize(224), T.CenterCrop(224), T.ToTensor(), imagenet_normalization()])\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    deterministic=True\n",
    ")\n",
    "trainer.test(resnet50, dataloaders=DataLoader(test_set, batch_size=128, num_workers=8, pin_memory=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageListDatasetWithBB(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_list, bb_list, transform=None):\n",
    "        super().__init__()\n",
    "        self.img_list = image_list\n",
    "        self.bb_list = bb_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_pth = self.img_list[idx]\n",
    "        img = PILImage.open(img_pth).convert(\"RGB\")\n",
    "        (x, y, w, h) = self.bb_list[idx]\n",
    "        img = img.crop((x, y, x+w, y+h))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "def convert_to_lists(image_pths, face_bbs):\n",
    "    input_img_list = []\n",
    "    input_face_bb_list = []\n",
    "    for img_pth, faces in zip(image_pths, face_bbs):\n",
    "        for bb in faces:\n",
    "            input_img_list.append(img_pth)\n",
    "            input_face_bb_list.append(bb)\n",
    "    assert len(input_img_list) == len(input_face_bb_list)\n",
    "\n",
    "    return input_img_list, input_face_bb_list\n",
    "\n",
    "def convert_to_df(predicted_classes, face_bbs):\n",
    "    df_rows = []\n",
    "    prediction_index = 0\n",
    "    for faces in face_bbs:\n",
    "        df_rows.append(predicted_classes[prediction_index:prediction_index+len(faces)])\n",
    "        prediction_index += len(faces)\n",
    "\n",
    "    # TODO: check return value\n",
    "    return df_rows    \n",
    "\n",
    "\n",
    "def get_predictions(model, dataframe):\n",
    "    input_img_list, input_face_bb_list = convert_to_lists(dataframe['image_path'], dataframe['face_bbs'])\n",
    "    dataset = ImageListDatasetWithBB(input_img_list, input_face_bb_list, transform=T.Compose([T.Resize(224), T.CenterCrop(224), T.ToTensor(), imagenet_normalization()]))\n",
    "    dataloader = DataLoader(dataset, batch_size=1024, num_workers=16, pin_memory=device=='cuda')\n",
    "\n",
    "    predicted_classes = []\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Predicting batches'):\n",
    "            batch = batch.to(device)\n",
    "            preds = resnet50(batch).cpu().argmax(-1)\n",
    "            predicted_classes.extend(np.array(test_set.classes)[preds])\n",
    "\n",
    "    return convert_to_df(predicted_classes, dataframe['face_bbs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_actors_predictions = get_predictions(resnet50, actors_df)\n",
    "resnet50_actresses_predictions = get_predictions(resnet50, actresses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_df['resnet50_predictions'] = resnet50_actors_predictions\n",
    "actresses_df['resnet50_predictions'] = resnet50_actresses_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anaylize the predictions of the CLIP and the ResNet50 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: uncomment if you run the notebook for the first time. The operations below take some time.\n",
    "# concat_df = pd.concat([actors_df, actresses_df], ignore_index=True)\n",
    "# concat_df['openclip_prediction_correct'] = concat_df['name'] == concat_df['openclip_prediction']\n",
    "# concat_df['resnet50_prediction_correct'] = concat_df.apply(lambda x: x['name'] in x['resnet50_predictions'], axis=1)\n",
    "# concat_df.to_csv('cc3m_experiments/facescrub_top50_similar_images_cc_VitB32.csv')\n",
    "concat_df = pd.read_csv('cc3m_experiments/facescrub_top50_similar_images_cc_VitB32.csv', index_col=0)\n",
    "concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_counts = concat_df.groupby('name').count().sort_values('image', ascending=False)\n",
    "actors_counts['count'] = actors_counts['image']\n",
    "actors_counts = actors_counts.drop(['image', 'similarity', 'id', 'image_path', 'openclip_prediction', 'gender', 'face_bbs', 'resnet50_predictions', 'openclip_prediction_correct', 'resnet50_prediction_correct'], axis=1).reset_index()\n",
    "print('Number of similar images per actor/actress')\n",
    "display(actors_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter all images where no face could be detected\n",
    "preds_df = concat_df[concat_df['face_bbs'].map(len) > 0].reset_index(drop=True)\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = preds_df.groupby('name').name.count().to_frame(name='num_similar_samples').reset_index()\n",
    "prediction_df['openclip_num_correct_preds'] = concat_df.groupby('name').openclip_prediction_correct.value_counts().unstack(fill_value=0).reset_index()[True]\n",
    "prediction_df['resnet50_num_correct_preds'] = concat_df.groupby('name').resnet50_prediction_correct.value_counts().unstack(fill_value=0).reset_index()[True]\n",
    "prediction_df['openclip_percentage_correct_preds'] = prediction_df['openclip_num_correct_preds'] / prediction_df['num_similar_samples']\n",
    "prediction_df['resnet50_percentage_correct_preds'] = prediction_df['resnet50_num_correct_preds'] / prediction_df['num_similar_samples']\n",
    "prediction_df['gender'] = concat_df.groupby('name').gender.value_counts().to_frame().rename(columns={'gender': 'num_samples'}).reset_index()['gender']\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv that contains the top 50 similar images of the laion 400M dataset to each of the facescrub images\n",
    "laion_similar_images_to_facescrub_actors = pd.read_csv(\"laion400m_experiments/facescrub_top200_similar_laion400m_images_actors.csv\", index_col=0)\n",
    "laion_similar_images_to_facescrub_actresses = pd.read_csv(\"laion400m_experiments/facescrub_top200_similar_laion400m_images_actresses.csv\", index_col=0)\n",
    "laion_similar_images_to_facescrub = pd.concat([laion_similar_images_to_facescrub_actors, laion_similar_images_to_facescrub_actresses], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the persons where the caption contains the names and count how many images there are with their name in the caption\n",
    "images_with_names = laion_similar_images_to_facescrub[laion_similar_images_to_facescrub['caption_contains_name']]\n",
    "num_images_with_names = images_with_names.groupby('class_name').caption_contains_name.value_counts().unstack(fill_value=0).reset_index().set_index('class_name')[True].reset_index()\n",
    "num_images_with_names = num_images_with_names.sort_values(True, ascending=False, ignore_index=True).rename(columns={'class_name': 'name', True: 'num_samples_with_name_in_cap'}).reset_index(drop=True)\n",
    "num_images_with_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the persons where there is a name in the caption from the predictions on the cc dataset\n",
    "inner_join = pd.merge(num_images_with_names, prediction_df, on='name')\n",
    "inner_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_join.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out those persons who have a higher correct prediction percentage than 20%\n",
    "cc_non_members = inner_join[(inner_join['openclip_percentage_correct_preds'] <= 0.20) & (inner_join['resnet50_percentage_correct_preds'] <= 0.15)]\n",
    "# filter out those persons who have less than 100 images with their name in the caption\n",
    "cc_non_members = cc_non_members[cc_non_members['num_samples_with_name_in_cap'] >= 100]\n",
    "cc_non_members = cc_non_members.sort_values(['resnet50_percentage_correct_preds'], ascending=True, ignore_index=True).groupby('gender').head(100).reset_index(drop=True)\n",
    "cc_non_members['openclip_percentage_correct_preds_bins'] = pd.qcut(cc_non_members['openclip_percentage_correct_preds'], q=20)\n",
    "cc_non_members['resnet50_percentage_correct_preds_bins'] = pd.qcut(cc_non_members['resnet50_percentage_correct_preds'], q=20)\n",
    "cc_non_members = cc_non_members.sort_values(['gender', 'openclip_percentage_correct_preds_bins', 'resnet50_percentage_correct_preds_bins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_non_members = cc_non_members.groupby('gender').get_group('m')\n",
    "# save the first half of the actors as non-members\n",
    "actor_non_members = actor_non_members[:int(len(actor_non_members)/2)].reset_index(drop=True)\n",
    "actor_non_members.to_csv('cc3m_experiments/conceptual_captions_facescrub_member_info/actors_non_members.csv')\n",
    "actor_non_members.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_non_members = cc_non_members.groupby('gender').get_group('m')\n",
    "# save the last half of the actors as members\n",
    "actor_non_members = actor_non_members[int(len(actor_non_members)/2):].reset_index(drop=True)\n",
    "actor_non_members.to_csv('cc3m_experiments/conceptual_captions_facescrub_member_info/actors_members.csv')\n",
    "actor_non_members.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actresses_non_members = cc_non_members.groupby('gender').get_group('f')\n",
    "# save the first half of the actors as non-members\n",
    "actresses_non_members = actresses_non_members[:int(len(actresses_non_members)/2)].reset_index(drop=True)\n",
    "actresses_non_members.to_csv('conceptual_captions_facescrub_member_info/actresses_non_members.csv')\n",
    "actresses_non_members.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actresses_non_members = cc_non_members.groupby('gender').get_group('f')\n",
    "# save the last half of the actors as members\n",
    "actresses_non_members = actresses_non_members[int(len(actresses_non_members)/2):].reset_index(drop=True)\n",
    "actresses_non_members.to_csv('cc3m_experiments/conceptual_captions_facescrub_member_info/actresses_members.csv')\n",
    "actresses_non_members.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how the percentage of correct predictions is for the actors\n",
    "cc_non_members.groupby('gender').get_group('f').tail(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
