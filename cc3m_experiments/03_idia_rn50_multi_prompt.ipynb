{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/workspace')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"3\"\n",
    "\n",
    "import torch\n",
    "import open_clip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import entropy\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "\n",
    "\n",
    "from rtpt.rtpt import setproctitle\n",
    "setproctitle('@Clip_Notebook')\n",
    "\n",
    "from datasets import FaceScrub, SingleClassSubset\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 15)\n",
    "\n",
    "# init clip\n",
    "RUN_PATHS = {\n",
    "    'top75': {'model_path': 'cc3m_experiments/checkpoints/rn50_top75_epoch_50.pt'},\n",
    "    'top50': {'model_path': 'cc3m_experiments/checkpoints/rn50_top50_epoch_50.pt'},\n",
    "    'top25': {'model_path': 'cc3m_experiments/checkpoints/rn50_top25_epoch_50.pt'},\n",
    "    'top10': {'model_path': 'cc3m_experiments/checkpoints/rn50_top10_epoch_50.pt'},\n",
    "    'top5': {'model_path': 'cc3m_experiments/checkpoints/rn50_top05_epoch_50.pt'},\n",
    "    'top1': {'model_path': 'cc3m_experiments/checkpoints/rn50_top01_epoch_50.pt'}\n",
    "}\n",
    "MODEL_NAME = 'RN50'\n",
    "\n",
    "DATASET_NAME = 'CC2M'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_TOTAL_NAMES = 1_000\n",
    "PROMPTS = [\n",
    "    '{0}',\n",
    "    'an image of {0}', \n",
    "    'a photo of {0}', \n",
    "    '{0} on a photo', \n",
    "    'a photo of a person named {0}', \n",
    "    'a person named {0}', \n",
    "    'a man named {0}',\n",
    "    'a woman named {0}',\n",
    "    'the name of the person is {0}', \n",
    "    'a photo of a person with the name {0}', \n",
    "    '{0} at a gala', \n",
    "    'a photo of the celebrity {0}', \n",
    "    'actor {0}',\n",
    "    'actress {0}',\n",
    "    'a colored photo of {0}',\n",
    "    'a black and white photo of {0}',\n",
    "    'a cool photo of {0}',\n",
    "    'a cropped photo of {0}',\n",
    "    'a cropped image of {0}',\n",
    "    '{0} in a suit',\n",
    "    '{0} in a dress'\n",
    "]\n",
    "MIN_NUM_IMAGES_AVAILABLE = 30 # the number of samples for a person that need to be available in order to consider it in the experiments. Persons with less will not be included in the experiments.\n",
    "MIN_NUM_CORRECT_PROMPT_PREDS = 1 # the number of prompts for which the majority prediction has to be correct (tau in the paper)\n",
    "\n",
    "SEED = 42\n",
    "LOAD_PREDICTION_METRICS_FROM_FILE = True\n",
    "LOAD_PREDICTIONS_FROM_FILE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "for num_members in RUN_PATHS.keys():\n",
    "    # pretrained_model = wandb.restore(name=RUN_PATHS[num_members]['model_path'], run_path=RUN_PATHS[num_members]['run_path'])\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        model_name=MODEL_NAME,\n",
    "        pretrained=RUN_PATHS[num_members]['model_path'],\n",
    "        precision='amp'\n",
    "    )\n",
    "    model = model.eval()\n",
    "    # only append the model since the preprocessing is the same for all the models since they are all the same model type\n",
    "    models[num_members] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get the predictions for an actor/actress\n",
    "@torch.no_grad()\n",
    "def get_text_embeddings(model, context, context_batchsize=10_000, use_tqdm=False):\n",
    "    context_batchsize = context_batchsize * torch.cuda.device_count()\n",
    "    # if there is not batches for the context unsqueeze it\n",
    "    if context.dim() < 3:\n",
    "        context = context.unsqueeze(0)\n",
    "\n",
    "    # get the batch size, the number of labels and the sequence length\n",
    "    seq_len = context.shape[-1]\n",
    "    viewed_context = context.view(-1, seq_len)\n",
    "\n",
    "    text_features = []\n",
    "    for context_batch_idx in tqdm(range(0, len(viewed_context), context_batchsize), desc=\"Calculating Text Embeddings\", disable=not use_tqdm):\n",
    "        context_batch = viewed_context[context_batch_idx:context_batch_idx + context_batchsize]\n",
    "        batch_text_features= model.encode_text(context_batch, normalize=True).cpu()\n",
    "\n",
    "        text_features.append(batch_text_features)\n",
    "    text_features = torch.cat(text_features).view(list(context.shape[:-1]) + [-1])\n",
    "\n",
    "    return text_features\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_preds_for_dataset(model, subset, context, batch_size=8, num_workers=8, device=device, context_batchsize=10_000, no_tqdm=False, text_embeddings=None):\n",
    "    dataloader = DataLoader(subset, batch_size=batch_size, num_workers=num_workers, pin_memory=device == 'cuda')\n",
    "\n",
    "    if text_embeddings is None:\n",
    "        text_embeddings = get_text_embeddings(model, context, context_batchsize=context_batchsize)\n",
    "\n",
    "    preds = []\n",
    "    for x, _ in tqdm(dataloader, desc='Iterating Dataset', disable=no_tqdm):\n",
    "        x = x.to(device)\n",
    "        image_features = model.encode_image(x, normalize=True).cpu()\n",
    "\n",
    "        image_features = image_features.unsqueeze(0)\n",
    "\n",
    "        # we have to calculate the cosine similarity manually. OpenAI does this internally.\n",
    "        logits_per_image = model.logit_scale.exp().cpu()  * image_features @ text_embeddings.swapaxes(-1, -2)\n",
    "        preds.append(logits_per_image.argmax(-1))\n",
    "\n",
    "    return torch.cat(preds, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Member and the Non-Member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the non-members\n",
    "fs_actors_non_members = pd.read_csv(\n",
    "    'conceptual_captions_experiment/conceptual_captions_facescrub_member_info/actors_non_members.csv', \n",
    "    index_col=0\n",
    ").rename(columns={'name': 'class_name'})\n",
    "fs_actors_non_members['name'] = fs_actors_non_members['class_name'].map(lambda x: x.replace('_', ' '))\n",
    "\n",
    "fs_actresses_non_members = pd.read_csv(\n",
    "    'conceptual_captions_experiment/conceptual_captions_facescrub_member_info/actresses_non_members.csv', \n",
    "    index_col=0\n",
    ").rename(columns={'name': 'class_name'})\n",
    "fs_actresses_non_members['name'] = fs_actresses_non_members['class_name'].map(lambda x: x.replace('_', ' '))\n",
    "\n",
    "# load the members\n",
    "fs_actors_members = pd.read_csv(\n",
    "    'conceptual_captions_experiment/conceptual_captions_facescrub_member_info/actors_members.csv', \n",
    "    index_col=0\n",
    ").rename(columns={'name': 'class_name'})\n",
    "fs_actors_members['name'] = fs_actors_members['class_name'].map(lambda x: x.replace('_', ' '))\n",
    "\n",
    "fs_actresses_members = pd.read_csv(\n",
    "    'conceptual_captions_experiment/conceptual_captions_facescrub_member_info/actresses_members.csv', \n",
    "    index_col=0\n",
    ").rename(columns={'name': 'class_name'})\n",
    "fs_actresses_members['name'] = fs_actresses_members['class_name'].map(lambda x: x.replace('_', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_members = pd.concat([fs_actors_members, fs_actresses_members], ignore_index=True)\n",
    "cc_non_members = pd.concat([fs_actors_non_members, fs_actresses_non_members], ignore_index=True)\n",
    "cc_individuals = pd.concat([cc_members, cc_non_members], ignore_index=True)\n",
    "cc_individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and load the FaceScrub Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facescrub = FaceScrub(root='./data/facescrub', group='all', train=True, cropped=False, transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_class_subsets = []\n",
    "for class_idx in range(len(facescrub.classes)):\n",
    "    dataset_class_subsets.append(SingleClassSubset(facescrub, class_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize an example\n",
    "plt.imshow(dataset_class_subsets[facescrub.class_to_idx[cc_members['class_name'][0]]][2][0].permute(1,2,0).numpy())\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Random First and Last Names to have more Possible Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the first names\n",
    "# list was taken from https://github.com/hadley/data-baby-names/blob/master/baby-names.csv which contains the top 1k names for the years 1880-2008 released by the US social security administration\n",
    "first_names_df = pd.read_csv('./conceptual_captions_experiment/common_first_names.csv')\n",
    "first_names_df = first_names_df.drop(columns=['year']).drop_duplicates(['name', 'sex'])\n",
    "first_names_df['sex'] = first_names_df['sex'].apply(lambda x: 'm' if x == 'boy' else 'f')\n",
    "# take the top 1k male and female names\n",
    "first_names_df = first_names_df.sort_values('percent', ascending=False).groupby('sex').head(1000).reset_index(drop=True).drop(columns=['percent']).rename(columns={'name': 'first_name'})\n",
    "first_names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the last names\n",
    "# list was taken from the US census burea at https://www.census.gov/topics/population/genealogy/data/2010_surnames.html and contains the top 1k surnames\n",
    "last_names_df = pd.read_csv('./conceptual_captions_experiment/common_last_names_US_2010.csv').dropna()[['SURNAME', 'FREQUENCY (COUNT)']]\n",
    "last_names_df = last_names_df.rename(columns={'SURNAME': 'last_name', 'FREQUENCY (COUNT)': 'count'})\n",
    "last_names_df['last_name'] = last_names_df['last_name'].str.title()\n",
    "last_names_df['count'] = last_names_df['count'].str.replace(',', '').astype(int)\n",
    "last_names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the cross product of the first and last names\n",
    "full_names_df = pd.merge(first_names_df[['first_name', 'sex']], last_names_df['last_name'], how='cross')\n",
    "# sample as much names from each gender equally as we need\n",
    "sampled_full_names_df = full_names_df.groupby('sex').sample(int((NUM_TOTAL_NAMES - len(facescrub.classes)) / 2), random_state=SEED).reset_index()\n",
    "sampled_full_names_list = sampled_full_names_df.apply(lambda x: f'{x[\"first_name\"]} {x[\"last_name\"]}', axis=1).tolist()\n",
    "print(f'Length List: {len(sampled_full_names_list)}')\n",
    "sampled_full_names_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the names from facescrub with the sampled names and shuffle them\n",
    "possible_names = [x.replace(\"_\", \" \") for x in facescrub.classes] + sampled_full_names_list\n",
    "print(possible_names[:10])\n",
    "possible_names = random.sample(possible_names, k=len(possible_names))\n",
    "print(f'Length Possible Names: {len(possible_names)}')\n",
    "possible_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Model for Test Purposes on the first Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare and fill the prompt templates\n",
    "prompts = []\n",
    "for name in possible_names:\n",
    "    df_dict = {}\n",
    "    for prompt_idx, prompt in enumerate(PROMPTS):\n",
    "        df_dict['class_name'] = \"_\".join(name.split(\" \"))\n",
    "        df_dict[f'prompt_{prompt_idx}'] = prompt.format(name)\n",
    "    prompts.append(df_dict)\n",
    "prompts = pd.DataFrame(prompts)\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the context vector of the possible labels\n",
    "if not LOAD_PREDICTIONS_FROM_FILE:\n",
    "    label_context_vecs = []\n",
    "    for i in range(len(PROMPTS)):\n",
    "        context = open_clip.tokenize(prompts[f'prompt_{i}'].to_numpy())\n",
    "        label_context_vecs.append(context)\n",
    "    label_context_vecs = torch.stack(label_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the embeddings for each of the models\n",
    "if not LOAD_PREDICTIONS_FROM_FILE:\n",
    "    label_context_vecs = label_context_vecs.to(device)\n",
    "\n",
    "    text_embeddings_per_model = {}\n",
    "    for num_members, model in models.items():\n",
    "        model = model.to(device)\n",
    "        text_embeddings = get_text_embeddings(model, label_context_vecs, use_tqdm=True)\n",
    "        text_embeddings_per_model[num_members] = text_embeddings\n",
    "        model = model.cpu()\n",
    "\n",
    "    label_context_vecs = label_context_vecs.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREDICTIONS_FROM_FILE:\n",
    "    test_subset_dataset = dataset_class_subsets[facescrub.class_to_idx[cc_members['class_name'][0]]]\n",
    "    for num_members, model in models.items():\n",
    "        model = model.to(device)\n",
    "        preds = get_preds_for_dataset(model, test_subset_dataset, label_context_vecs, num_workers=2, text_embeddings=text_embeddings_per_model[num_members])\n",
    "        unique_vals, counts = [], []\n",
    "        for x in preds:\n",
    "            x = x.unique(return_counts=True)\n",
    "            unique_vals.append(x[0])\n",
    "            counts.append(x[1])\n",
    "        model = model.cpu()\n",
    "        predictions = [int(vals[count.topk(1, sorted=True)[1]]) for vals, count in zip(unique_vals, counts)]\n",
    "        print(f'Prediction Model {num_members}: {prompts[\"class_name\"].iloc[predictions].to_list()}\\t Correct Class: {facescrub.classes[test_subset_dataset.target_class]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREDICTIONS_FROM_FILE:\n",
    "    test_subset_dataset = dataset_class_subsets[facescrub.class_to_idx[cc_non_members['class_name'][0]]]\n",
    "    for num_members, model in models.items():\n",
    "        model = model.to(device)\n",
    "        preds = get_preds_for_dataset(model, test_subset_dataset, label_context_vecs, num_workers=2, text_embeddings=text_embeddings_per_model[num_members])\n",
    "        unique_vals, counts = [], []\n",
    "        for x in preds:\n",
    "                x = x.unique(return_counts=True)\n",
    "                unique_vals.append(x[0])\n",
    "                counts.append(x[1])\n",
    "        model = model.cpu()\n",
    "        predictions = [int(vals[count.topk(1, sorted=True)[1]]) for vals, count in zip(unique_vals, counts)]\n",
    "        print(f'Prediction Model {num_members}: {prompts[\"class_name\"].iloc[predictions].to_list()}\\t Correct Class: {facescrub.classes[test_subset_dataset.target_class]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the CLIP model on all Actors\n",
    "Filter for (Non-)Members afterwards using Pands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREDICTIONS_FROM_FILE:\n",
    "    filtered_subsets = []\n",
    "    for subset in dataset_class_subsets:\n",
    "        if facescrub.classes[subset.target_class] in cc_individuals['class_name'].tolist():\n",
    "            filtered_subsets.append(subset)\n",
    "\n",
    "    concat_dataset = ConcatDataset([subset for subset in filtered_subsets])\n",
    "    preds_per_model = {}\n",
    "    for num_members, model in models.items():\n",
    "        model = model.to(device)\n",
    "        preds = get_preds_for_dataset(model, concat_dataset, label_context_vecs, batch_size=128, num_workers=32, text_embeddings=text_embeddings_per_model[num_members])\n",
    "        model = model.cpu()\n",
    "        assert preds.shape[1] == len(concat_dataset)\n",
    "        assert preds.shape[0] == len(PROMPTS)\n",
    "        # transpose the predictions such that we have len(PROMPTS) predictions for each sample\n",
    "        preds = preds.T\n",
    "        preds_per_model[num_members] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the large list of all predictions into prediction lists for every class\n",
    "if not LOAD_PREDICTIONS_FROM_FILE:\n",
    "    preds_per_model_per_subset = {}\n",
    "    for num_members, preds in preds_per_model.items():\n",
    "        preds_per_subset = []\n",
    "        counter = 0\n",
    "        for subset in filtered_subsets:\n",
    "            subset_preds = preds[counter:counter + len(subset)]\n",
    "            assert len(subset_preds) == len(subset)\n",
    "            preds_per_subset.append(subset_preds)\n",
    "            counter += len(subset)\n",
    "        preds_per_model_per_subset[num_members] = preds_per_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREDICTIONS_FROM_FILE:\n",
    "    preds_df_per_model = {}\n",
    "    for num_members in models.keys():\n",
    "        df_list = []\n",
    "        for group_idx, (dataset_subset, preds_subset) in enumerate(zip(filtered_subsets, preds_per_model_per_subset[num_members])):\n",
    "            for sample_idx, pred in enumerate(preds_subset):\n",
    "                class_name = facescrub.classes[filtered_subsets[group_idx].target_class]\n",
    "                result_dict = {\n",
    "                    'group_idx': group_idx,\n",
    "                    'class_name': class_name,\n",
    "                    'sample_idx': sample_idx\n",
    "                }\n",
    "                for i, pred_idx in enumerate(pred):\n",
    "                    result_dict[f'name_prediction_prompt_{i}'] = prompts['class_name'].iloc[int(pred_idx)]\n",
    "                df_list.append(result_dict)\n",
    "        preds_df = pd.DataFrame(df_list)\n",
    "        preds_df_per_model[num_members] = preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREDICTIONS_FROM_FILE:\n",
    "    display(preds_df_per_model['top75'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only get the rows of the members and non-members\n",
    "if not LOAD_PREDICTIONS_FROM_FILE:\n",
    "    for num_members, preds_df in preds_df_per_model.items():\n",
    "        members = pd.merge(preds_df, cc_members['class_name'], on='class_name')\n",
    "        members['actual_membership'] = 'member'\n",
    "        non_members = pd.merge(preds_df, cc_non_members['class_name'], on='class_name')\n",
    "        non_members['actual_membership'] = 'non_member'\n",
    "        preds_df_per_model[num_members] = pd.concat([members, non_members])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the predictions to file if necessary to prevent long runtimes\n",
    "if not LOAD_PREDICTIONS_FROM_FILE:\n",
    "    for num_members, preds_df in preds_df_per_model.items():\n",
    "        preds_df.to_csv(f'conceptual_captions_experiment/prediction_dfs/predictions_multiprompt_{DATASET_NAME}_{MODEL_NAME}_{num_members}_{NUM_TOTAL_NAMES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.csv')\n",
    "else:\n",
    "    preds_df_per_model = {}\n",
    "    for num_members, preds_df in models.items():\n",
    "        preds_df_per_model[num_members] = pd.read_csv(f'conceptual_captions_experiment/prediction_dfs/predictions_multiprompt_{DATASET_NAME}_{MODEL_NAME}_{num_members}_{NUM_TOTAL_NAMES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df_per_model['top75']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_sizes_per_model = {}\n",
    "for num_members, preds_df in preds_df_per_model.items():\n",
    "    subsample_sizes = np.arange(1, MIN_NUM_IMAGES_AVAILABLE+1, 2).tolist()\n",
    "    subsample_sizes.append(30)\n",
    "    subsample_sizes_per_model[num_members] = subsample_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_membership_metrics(df, sample_size, sample_draws):\n",
    "    subsampled_metrics_dfs = []\n",
    "    for i in range(sample_draws):\n",
    "        # sample the same number of images/predictions for each person\n",
    "        name_predictions_df = preds_df.groupby('class_name').sample(sample_size).reset_index(drop=True)\n",
    "\n",
    "        # get the number of members and non_members\n",
    "        num_member, num_non_member = name_predictions_df[['class_name', 'actual_membership']].drop_duplicates()['actual_membership'].value_counts()\n",
    "\n",
    "        # get the column names of the different prompts\n",
    "        prompt_column_names = [f'name_prediction_prompt_{i}' for i in range(len(PROMPTS))]\n",
    "\n",
    "        def get_name_predictions(predictions: pd.Series, values_only=False, counts_only=False):\n",
    "            \"\"\"Takes a series of predictions and returns the unique values and the number of prediction occurrences in descending order.\"\"\"\n",
    "            values, counts = np.unique(predictions, return_counts=True)\n",
    "            descending_counts_indices = counts.argsort()[::-1]\n",
    "\n",
    "            if values_only:\n",
    "                return values[descending_counts_indices]\n",
    "            elif counts_only:\n",
    "                return counts[descending_counts_indices]\n",
    "            else:\n",
    "                return values[descending_counts_indices], counts[descending_counts_indices]\n",
    "\n",
    "        name_prediction_count_df = name_predictions_df.groupby('class_name')[prompt_column_names].agg(list)\n",
    "        name_prediction_count_df[[f'unique_name_predictions_prompt_{i}' for i in range(len(prompt_column_names))]] = name_prediction_count_df[prompt_column_names].apply(lambda x: x.apply(lambda y: get_name_predictions(y, values_only=True)))\n",
    "        name_prediction_count_df[[f'unique_name_prediction_count_prompt_{i}' for i in range(len(prompt_column_names))]] = name_prediction_count_df[prompt_column_names].apply(lambda x: x.apply(lambda y: get_name_predictions(y, counts_only=True)))\n",
    "\n",
    "        # get the actual membership by merging with the sampled dataframe\n",
    "        name_prediction_count_df = pd.merge(name_prediction_count_df, name_predictions_df[['class_name', 'actual_membership']].drop_duplicates().set_index('class_name'), how='inner', on='class_name')\n",
    "\n",
    "        def check_for_correct_prompt_majority(row: pd.Series):\n",
    "            \"\"\"Takes a row of the dataframe and checks whether the correct name was predicted the majority of the time.\"\"\"\n",
    "            # iterate the prompts\n",
    "            num_correct_prompts = 0\n",
    "            for prompt_idx in range(len(row[prompt_column_names])):\n",
    "                unique_predictions = row[f'unique_name_predictions_prompt_{prompt_idx}']\n",
    "                prediction_counts = row[f'unique_name_prediction_count_prompt_{prompt_idx}']\n",
    "                \n",
    "                # get the indices of the most often predicted names\n",
    "                idx_most_often_pred_names = np.argwhere(prediction_counts == prediction_counts.max()).flatten()\n",
    "\n",
    "                # if there are two or more names predicted the same time, we don't have a clear majority prediction and therefore skip this prompt\n",
    "                if len(idx_most_often_pred_names) > 1:\n",
    "                    continue\n",
    "\n",
    "                # if a name was predicted by the majority and it is the correct name, we have a correct majority prediction\n",
    "                if unique_predictions[idx_most_often_pred_names[0]] == row.name:\n",
    "                    assert len(idx_most_often_pred_names) == 1\n",
    "                    num_correct_prompts += 1\n",
    "\n",
    "            # return true if the number of prompts is greater or equal to the threshold\n",
    "            return num_correct_prompts >= MIN_NUM_CORRECT_PROMPT_PREDS\n",
    "\n",
    "        name_prediction_count_df['correct_majority_prediction'] = name_prediction_count_df.apply(check_for_correct_prompt_majority, axis=1)\n",
    "        name_prediction_count_df['membership_prediction'] = name_prediction_count_df['correct_majority_prediction'].apply(lambda x: 'member' if x else 'non_member')\n",
    "        name_prediction_count_df['sample_size'] = sample_size\n",
    "        name_prediction_count_df['draw'] = i\n",
    "\n",
    "        tp = len(name_prediction_count_df[(name_prediction_count_df['membership_prediction'] == 'member') & (name_prediction_count_df['actual_membership'] == 'member')])\n",
    "        fp = len(name_prediction_count_df[(name_prediction_count_df['membership_prediction'] == 'member') & (name_prediction_count_df['actual_membership'] == 'non_member')])\n",
    "        fn = len(name_prediction_count_df[(name_prediction_count_df['membership_prediction'] == 'non_member') & (name_prediction_count_df['actual_membership'] == 'member')])\n",
    "        tn = len(name_prediction_count_df[(name_prediction_count_df['membership_prediction'] == 'non_member') & (name_prediction_count_df['actual_membership'] == 'non_member')])\n",
    "\n",
    "        subsampled_metrics_dfs.append({\n",
    "            'sample_size': sample_size,\n",
    "            'draw': i,\n",
    "            'tpr': tp / num_member,\n",
    "            'fnr': fn / num_member,\n",
    "            'fpr': fp / num_non_member,\n",
    "            'tnr': tn / num_non_member,\n",
    "            'tp': tp,\n",
    "            'fn': fn,\n",
    "            'fp': fp,\n",
    "            'tn': tn\n",
    "        })\n",
    "    \n",
    "    return subsampled_metrics_dfs\n",
    "\n",
    "\n",
    "class TQDMParallel(Parallel):\n",
    "    def __init__(self, progress_bar=True, total=None, *args, **kwargs):\n",
    "        self.progress_bar = progress_bar\n",
    "        self.total = total\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        with tqdm(disable=not self.progress_bar, total=self.total) as self.pbar:\n",
    "            return Parallel.__call__(self, *args, **kwargs)\n",
    "\n",
    "    def print_progress(self):\n",
    "        if self.total is None:\n",
    "            self.pbar.total = self.n_dispatched_tasks\n",
    "        self.pbar.n = self.n_completed_tasks\n",
    "        self.pbar.refresh()\n",
    "\n",
    "if not LOAD_PREDICTION_METRICS_FROM_FILE:\n",
    "    subsampled_dfs_per_model = {}\n",
    "    sample_draws = 20\n",
    "    for num_member_identities, preds_df in preds_df_per_model.items():\n",
    "        subsample_sizes = subsample_sizes_per_model[num_member_identities]\n",
    "        arguments_list = []\n",
    "        for sample_size in subsample_sizes:\n",
    "            arguments_list.append((preds_df, sample_size, sample_draws))\n",
    "\n",
    "        print(f'{num_member_identities} with {len(arguments_list)} predictions')\n",
    "        subsampled_dfs = TQDMParallel(n_jobs=1, total=len(arguments_list))(\n",
    "            delayed(get_membership_metrics)(*arguments) for arguments in arguments_list\n",
    "        )\n",
    "\n",
    "        flattened_subsampled_dfs = []\n",
    "        [flattened_subsampled_dfs.extend(x) for x in subsampled_dfs]\n",
    "        subsampled_dfs_per_model[num_member_identities] = flattened_subsampled_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREDICTION_METRICS_FROM_FILE:\n",
    "    for num_members, subsampled_dfs in subsampled_dfs_per_model.items():\n",
    "        subsampled_dfs_per_model[num_members] = pd.DataFrame(subsampled_dfs_per_model[num_members]).set_index('sample_size').drop('draw', axis='columns')\n",
    "        subsampled_dfs_per_model[num_members] = subsampled_dfs_per_model[num_members].rename(columns={'tpr': 'True Positive Rate', 'fnr': 'False Negative Rate', 'fpr': 'False Positive Rate', 'tnr': 'True Negative Rate'})\n",
    "        subsampled_dfs_per_model[num_members].index.name = 'Number of Samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREDICTION_METRICS_FROM_FILE:\n",
    "    for num_members, subsampled_dfs in subsampled_dfs_per_model.items():\n",
    "        subsampled_dfs_per_model[num_members].to_csv(f'./conceptual_captions_experiment/prediction_metrics_dfs/multiprompt_prediction_metrics_{DATASET_NAME}_{MODEL_NAME}_{num_members}_{NUM_TOTAL_NAMES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.csv')\n",
    "else:\n",
    "    subsampled_dfs_per_model = {}\n",
    "    for num_members, model in models.items():\n",
    "        subsampled_dfs_per_model[num_members] = pd.read_csv(f'./conceptual_captions_experiment/prediction_metrics_dfs/multiprompt_prediction_metrics_{DATASET_NAME}_{MODEL_NAME}_{num_members}_{NUM_TOTAL_NAMES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_dfs_per_model['top75']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_members, df in subsampled_dfs_per_model.items():\n",
    "    print(num_members)\n",
    "    display(df.groupby('Number of Samples').mean().head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the accuracy for each sampling\n",
    "for num_members, subsampled_dfs in subsampled_dfs_per_model.items():\n",
    "    subsampled_dfs_per_model[num_members]['Accuracy'] = (subsampled_dfs['tp'] + subsampled_dfs['tn']) / (subsampled_dfs['tp'] + subsampled_dfs['tn'] + subsampled_dfs['fp'] + subsampled_dfs['fn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "show_y_axis = MODEL_NAME == 'RN50'\n",
    "show_legend = True\n",
    "for num_members, subsampled_dfs in subsampled_dfs_per_model.items():\n",
    "    plt.clf()\n",
    "    data = subsampled_dfs[['Accuracy', 'True Positive Rate', 'False Negative Rate', 'False Positive Rate', 'True Negative Rate']]\n",
    "    data = data.rename(columns={\n",
    "        'True Positive Rate': 'TPR', \n",
    "        'False Negative Rate': 'FNR', \n",
    "        'False Positive Rate': 'FPR', \n",
    "        'True Negative Rate': 'TNR', \n",
    "        'Accuracy': 'Acc'\n",
    "        }\n",
    "    )\n",
    "    ax = sns.lineplot(data=data, errorbar='sd', palette='colorblind')\n",
    "\n",
    "    ax.set_xlabel(\"Number of Images used for IDIA\", weight=\"bold\", size=16)\n",
    "    ax.set_xticks([i for i in range(0, data.index.unique().max()+1, 5)])\n",
    "    ax.set_xticklabels([int(x) for x in ax.get_xticks()], size=16)\n",
    "\n",
    "    h, l = ax.get_legend_handles_labels()\n",
    "    ax.set_yticks([i for i in np.arange(0, 1+0.1, 0.1)])\n",
    "    ax.set_yticklabels(ax.get_yticks(), size=16)\n",
    "    ax.legend(h, l, ncol=2, fontsize=16)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "    # show only every second y tick label\n",
    "    plt.setp(ax.yaxis.get_ticklabels()[1::2], visible=False)\n",
    "\n",
    "    if show_legend:\n",
    "        h, l = ax.get_legend_handles_labels()\n",
    "        ax.legend(h, l, ncol=2, fontsize=16)\n",
    "    else:\n",
    "        ax.legend_.remove()\n",
    "\n",
    "    if not show_y_axis:\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    ax.get_figure().savefig(f'./conceptual_captions_experiment/plots/{MODEL_NAME}/num_idia_samples_plot_multiprompt_{DATASET_NAME}_{MODEL_NAME}_{num_members}_{NUM_TOTAL_NAMES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.pdf')\n",
    "    ax.get_figure().savefig(f'./conceptual_captions_experiment/plots/{MODEL_NAME}/num_idia_samples_plot_multiprompt_{DATASET_NAME}_{MODEL_NAME}_{num_members}_{NUM_TOTAL_NAMES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.png', dpi=100)\n",
    "    print(num_members)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_members, subsampled_dfs in subsampled_dfs_per_model.items():\n",
    "    tp_std, fn_std, fp_std, tn_std = subsampled_dfs.groupby('Number of Samples').std().iloc[-1][['tp', 'fn', 'fp', 'tn']]\n",
    "    tp, fn, fp, tn = subsampled_dfs.groupby('Number of Samples').mean().iloc[-1][['tp', 'fn', 'fp', 'tn']]\n",
    "\n",
    "    tpr_std, fnr_std, fpr_std, tnr_std = subsampled_dfs.groupby('Number of Samples').std().iloc[-1][['True Positive Rate', 'False Negative Rate', 'False Positive Rate', 'True Negative Rate']]\n",
    "    tpr, fnr, fpr, tnr = subsampled_dfs.groupby('Number of Samples').mean().iloc[-1][['True Positive Rate', 'False Negative Rate', 'False Positive Rate', 'True Negative Rate']]\n",
    "\n",
    "    normalized_conf_mat = pd.DataFrame({'Member': [tpr, fpr], 'Non-Member': [fnr, tnr]}, index=['Member', 'Non-Member'])\n",
    "    normalized_conf_mat.index.set_names('Actual Membership', inplace=True)\n",
    "    normalized_conf_mat = normalized_conf_mat.rename_axis('Predicted Membership', axis='columns')\n",
    "\n",
    "    group_names = ['TP','FN','FP','TN']\n",
    "    group_counts = [\"{0:0.0f} \\u00B1 {1:0.2f}\".format(mean, std) for mean, std in zip([tp, fn, fp, tn], [tp_std, fn_std, fp_std, tn_std])]\n",
    "    percentage = [\"{0:0.2f}% \\u00B1 {1:0.02f}%\".format(mean * 100, std * 100) for mean, std in zip([tpr, fnr, fpr, tnr], [tpr_std, fnr_std, fpr_std, tnr_std])]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names, group_counts, percentage)]\n",
    "    plt.clf()\n",
    "    ax = sns.heatmap(normalized_conf_mat, annot=np.asarray(labels).reshape(2, 2), fmt='', cbar=False, cmap='Blues', annot_kws={'fontsize': 16})\n",
    "\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), size=16)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), size=16)\n",
    "\n",
    "    plt.ylabel('Actual Membership', fontsize=16, weight='bold')\n",
    "    \n",
    "    plt.xlabel('Predicted Membership', fontsize=16, weight='bold')\n",
    "    plt.tight_layout()\n",
    "    ax.get_figure().savefig(f'./conceptual_captions_experiment/plots/{MODEL_NAME}/confusion_matrix_multiprompt_{DATASET_NAME}_{MODEL_NAME}_{num_members}_{NUM_TOTAL_NAMES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.pdf')\n",
    "    ax.get_figure().savefig(f'./conceptual_captions_experiment/plots/{MODEL_NAME}/confusion_matrix_multiprompt_{DATASET_NAME}_{MODEL_NAME}_{num_members}_{NUM_TOTAL_NAMES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.png', dpi=100)\n",
    "    print(num_members)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Metrics Against the Number of Samples in the Traing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_legend = True\n",
    "show_y_axis = True\n",
    "rows = []\n",
    "for num_members in subsampled_dfs_per_model.keys():\n",
    "    plt.clf()\n",
    "    data = subsampled_dfs_per_model[num_members].loc[:, ['True Positive Rate', 'False Negative Rate', 'False Positive Rate', 'True Negative Rate', 'Accuracy']]\n",
    "    data[\"Number of Images\\nper Person in CC3M\"] = int(num_members.replace(\"top\", \"\"))\n",
    "    # get the last group (30 attack samples) to calculate mean and std\n",
    "    rows.append(data.groupby('Number of Samples').get_group(data.groupby('Number of Samples').last().iloc[-1].name).set_index(\"Number of Images\\nper Person in CC3M\"))\n",
    "\n",
    "df = pd.concat(rows).rename(columns={\n",
    "    'True Positive Rate': 'TPR', \n",
    "    'False Negative Rate': 'FNR',\n",
    "    'False Positive Rate': 'FPR', \n",
    "    'True Negative Rate': 'TNR', \n",
    "    'Accuracy': 'Acc'\n",
    "    })\n",
    "\n",
    "# plot the graph\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "rows_to_plot = ['TPR', 'FNR']\n",
    "display(df[rows_to_plot].groupby(df.index).mean())\n",
    "ax = sns.lineplot(data=df[rows_to_plot], errorbar='sd')\n",
    "\n",
    "ax.set_xticks([int(x.replace(\"top\", \"\")) for x in subsampled_dfs_per_model.keys()][::-1])\n",
    "ax.set_xticklabels(ax.get_xticks(), size=16)\n",
    "\n",
    "ax.set_xlabel(df.index.name, weight='bold', size=16)\n",
    "\n",
    "# ax.set(yticklabels=[\" \" for x in ax.get_yticklabels()], ylabel=\" \")\n",
    "# ax.set_yticklabels([x.get_text() for x in ax.get_yticklabels()], weight=\"bold\", size=16)\n",
    "ax.set_yticks([i for i in np.arange(0, 1+0.1, 0.1)])\n",
    "ax.set_yticklabels(ax.get_yticks(), size=16)\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "ax.set_ylim(-0.025, 1.025)\n",
    "\n",
    "# show only every second y tick label\n",
    "plt.setp(ax.yaxis.get_ticklabels()[1::2], visible=False)\n",
    "\n",
    "if not show_y_axis:\n",
    "    ax.set_yticklabels([]) \n",
    "\n",
    "if show_legend:\n",
    "    h, l = ax.get_legend_handles_labels()\n",
    "    ax.legend(h, l, ncol=2, loc='lower center', fontsize=16)\n",
    "else:\n",
    "    ax.legend_.remove()\n",
    "\n",
    "# plt.tight_layout()\n",
    "ax.get_figure().savefig(f'./conceptual_captions_experiment/plots/{MODEL_NAME}/num_training_samples_plot_multiprompt_{DATASET_NAME}_{MODEL_NAME}_{NUM_TOTAL_NAMES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.pdf', bbox_inches='tight')\n",
    "ax.get_figure().savefig(f'./conceptual_captions_experiment/plots/{MODEL_NAME}/num_training_samples_plot_multiprompt_{DATASET_NAME}_{MODEL_NAME}_{NUM_TOTAL_NAMES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.png', dpi=100, bbox_inches='tight')\n",
    "print(MODEL_NAME)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot a Heatmap to Visualize Influence of Number of Attack/Trainin Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  matplotlib.ticker import FuncFormatter\n",
    "show_cbar = False\n",
    "show_ylabel = True\n",
    "dfs = []\n",
    "for num_members in subsampled_dfs_per_model.keys():\n",
    "    plt.clf()\n",
    "    data = subsampled_dfs_per_model[num_members].loc[:, ['True Positive Rate', 'False Negative Rate', 'False Positive Rate', 'True Negative Rate', 'Accuracy']]\n",
    "    data[\"Number of Images\\nper Person in CC3M\"] = int(num_members.replace(\"top\", \"\"))\n",
    "    data = data.groupby(\"Number of Samples\").mean()\n",
    "    dfs.append(data)\n",
    "\n",
    "combined_df = pd.concat(dfs)    \n",
    "pivoted_df = combined_df.reset_index().pivot(index=\"Number of Samples\", columns=\"Number of Images\\nper Person in CC3M\", values=\"True Positive Rate\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "if show_cbar:\n",
    "    plt.figure(figsize=(6.2, 5))\n",
    "\n",
    "ax = sns.heatmap(pivoted_df, yticklabels=2, vmin=0, vmax=1, cmap=\"Blues\", cbar=show_cbar, ax=ax)\n",
    "\n",
    "if show_cbar:\n",
    "    ax.figure.axes[-1].set_ylabel(\"True Positive Rate\", weight='bold', size=16)\n",
    "    ax.collections[0].colorbar.ax.tick_params(labelsize=16)\n",
    "\n",
    "ax.set_xlabel(ax.get_xlabel(), weight=\"bold\", size=16)\n",
    "ax.set_ylabel(ax.get_ylabel(), weight=\"bold\", size=16)\n",
    "\n",
    "ax.set_yticklabels([x.get_text() for x in ax.get_yticklabels()], size=16)\n",
    "ax.set_xticklabels([int(float(x.get_text())) for x in ax.get_xticklabels()], size=16)\n",
    "\n",
    "ax.invert_yaxis()\n",
    "\n",
    "if not show_ylabel:\n",
    "    ax.set(yticklabels=[\" \" for x in ax.get_yticklabels()], ylabel=\" \")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "ax.get_figure().savefig(f'./conceptual_captions_experiment/plots/{MODEL_NAME}/heatmap_num_training_samples_{DATASET_NAME}_{MODEL_NAME}_{NUM_TOTAL_NAMES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.pdf', bbox_inches='tight')\n",
    "ax.get_figure().savefig(f'./conceptual_captions_experiment/plots/{MODEL_NAME}/heatmap_num_training_samples_{DATASET_NAME}_{MODEL_NAME}_{NUM_TOTAL_NAMES}_{MIN_NUM_CORRECT_PROMPT_PREDS}.png', dpi=100, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
