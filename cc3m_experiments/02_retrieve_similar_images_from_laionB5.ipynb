{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from clip_retrieval.clip_client import ClipClient, Modality\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import Image, display\n",
    "import torch\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n",
    "from random import uniform\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image as PILImage\n",
    "import PIL\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "os.chdir('/workspace')\n",
    "from rtpt.rtpt import setproctitle\n",
    "setproctitle('@Clipping_Privacy_CC3M_Notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "QUERY_BACKEND = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load the csv files of the members ans non-members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load the non-members\n",
    "fs_actors_non_members = pd.read_csv(\n",
    "    'cc3m_experiments/conceptual_captions_facescrub_member_info/actors_non_members.csv', \n",
    "    index_col=0\n",
    ").rename(columns={'name': 'class_name'})\n",
    "fs_actors_non_members['name'] = fs_actors_non_members['class_name'].map(lambda x: x.replace('_', ' '))\n",
    "print('actors_non_members')\n",
    "display(fs_actors_non_members.head(3))\n",
    "fs_actresses_non_members = pd.read_csv(\n",
    "    'cc3m_experiments/conceptual_captions_facescrub_member_info/actresses_non_members.csv', \n",
    "    index_col=0\n",
    ").rename(columns={'name': 'class_name'})\n",
    "fs_actresses_non_members['name'] = fs_actresses_non_members['class_name'].map(lambda x: x.replace('_', ' '))\n",
    "print('actresses_non_members')\n",
    "display(fs_actresses_non_members.head(3))\n",
    "# load the members\n",
    "fs_actors_members = pd.read_csv(\n",
    "    'cc3m_experiments/conceptual_captions_facescrub_member_info/actors_members.csv', \n",
    "    index_col=0\n",
    ").rename(columns={'name': 'class_name'})\n",
    "fs_actors_members['name'] = fs_actors_members['class_name'].map(lambda x: x.replace('_', ' '))\n",
    "print('actors_members')\n",
    "display(fs_actors_members.head(3))\n",
    "fs_actresses_members = pd.read_csv(\n",
    "    'cc3m_experiments/conceptual_captions_facescrub_member_info/actresses_members.csv', \n",
    "    index_col=0\n",
    ").rename(columns={'name': 'class_name'})\n",
    "fs_actresses_members['name'] = fs_actresses_members['class_name'].map(lambda x: x.replace('_', ' '))\n",
    "print('actresses_members')\n",
    "display(fs_actresses_members.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Get similar images with the captions from the LAION-5B dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NUM_IMAGES_TO_QUERY_FOR=999\n",
    "MIN_NUM_IMGS_PER_PERSON=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "client = ClipClient(\n",
    "    url='https://knn5.laion.ai/knn-service',\n",
    "    indice_name='laion5B',\n",
    "    aesthetic_weight=0,\n",
    "    modality=Modality.IMAGE,\n",
    "    use_safety_model=False,\n",
    "    use_violence_detector=True,\n",
    "    deduplicate=True,\n",
    "    num_images=NUM_IMAGES_TO_QUERY_FOR\n",
    ")\n",
    "\n",
    "def log_result(result):\n",
    "    id, caption, url, similarity = result[\"id\"], result[\"caption\"], result[\"url\"], result[\"similarity\"]\n",
    "    print(f\"id: {id}\")\n",
    "    print(f\"caption: {caption}\")\n",
    "    print(f\"url: {url}\")\n",
    "    print(f\"similarity: {similarity}\")\n",
    "    display(Image(url=url, unconfined=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load the facescrub embeddings calculated with the openai clip model\n",
    "embedding_dict = torch.load('./embeddings/openai_facescrub.pt')\n",
    "\n",
    "class_list = np.array(embedding_dict['classes'])\n",
    "# remove the dot from Freddy Prinze Jr\n",
    "indices_freddy_prinze_jr = np.where(class_list == 'Freddy_Prinze_Jr.')\n",
    "class_list[indices_freddy_prinze_jr] = 'Freddy_Prinze_Jr'\n",
    "# fix typo in Leslie Nielsen\n",
    "indices_leslie_nielsen = np.where(class_list == 'Leslie_Neilsen')\n",
    "class_list[indices_leslie_nielsen] = 'Leslie_Nielsen'\n",
    "# fix typo in Robert De Niro\n",
    "indices_robert_de_niro = np.where(class_list == 'Robert_Di_Niro')\n",
    "class_list[indices_robert_de_niro] = 'Robert_De_Niro'\n",
    "# remove middle name from Tatyana Ali\n",
    "indices_tatyana_ali = np.where(class_list == 'Tatyana_M._Ali')\n",
    "class_list[indices_tatyana_ali] = 'Tatyana_Ali'\n",
    "embedding_dict['classes'] = class_list.tolist()\n",
    "\n",
    "# get the data as a df\n",
    "embeddings_df = pd.DataFrame({'class_name': embedding_dict['classes'], 'image_paths': embedding_dict['image_paths'], 'embeddings': [x for x in embedding_dict['embeddings'].numpy()]})\n",
    "embeddings_df['name'] = embeddings_df['class_name'].apply(lambda x: x.replace('_', ' '))\n",
    "embeddings_df.groupby('name').head(1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get only the actors/actresses that are used as members and non-members\n",
    "concat_dataset = pd.concat([fs_actors_members, fs_actors_non_members, fs_actresses_members, fs_actresses_non_members], ignore_index=True)\n",
    "chosen_persons_for_experiment = pd.merge(embeddings_df, concat_dataset['class_name'], on='class_name', how='inner')\n",
    "chosen_persons_for_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if QUERY_BACKEND:    \n",
    "    print(f'Testing on {chosen_persons_for_experiment[\"name\"][0]}')\n",
    "    test = client.query(text=chosen_persons_for_experiment['image_paths'][0])\n",
    "    print(len(test))\n",
    "    log_result(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_images(client, image_path):\n",
    "    res = []\n",
    "    try:\n",
    "        res = client.query(image=image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "    return res\n",
    "\n",
    "def filter_imgs_for_name_in_cap(name, res):\n",
    "    results = []\n",
    "    for result in res:\n",
    "        if name.lower() in result['caption'].lower():\n",
    "            result['name'] = name\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "def query_backend(name, image_paths, client_instance, min_num_images=MIN_NUM_IMGS_PER_PERSON):\n",
    "    result = []\n",
    "    pbar = tqdm(total=min_num_images, desc=f'Total Images Retrieved for {name}', leave=False)\n",
    "    for i, path in tqdm(enumerate(image_paths), total=len(image_paths), desc=f'Total Amount of Queries for {name}', leave=False):\n",
    "        res = get_images(client_instance, path)\n",
    "        sleep(uniform(1,5))\n",
    "        res = filter_imgs_for_name_in_cap(name, res)\n",
    "        client_instance.num_images = NUM_IMAGES_TO_QUERY_FOR\n",
    "\n",
    "        result.extend(res)\n",
    "        # filter out duplicate urls and duplicate captions\n",
    "        result = pd.DataFrame(result).drop_duplicates('url').drop_duplicates('caption').to_dict('records')\n",
    "        # update the progress bar\n",
    "        pbar.n = len(result)\n",
    "        pbar.refresh()\n",
    "        if len(result) >= min_num_images:\n",
    "            break\n",
    "\n",
    "    print(f'{len(result)} images found for {name}')\n",
    "    return result\n",
    "\n",
    "if QUERY_BACKEND:\n",
    "    groups = chosen_persons_for_experiment.groupby('name')\n",
    "    similar_images = []\n",
    "    for name, group in tqdm(groups, total=len(groups), desc='Total Progress'):\n",
    "        similar_images_for_group = query_backend(name, group['image_paths'], client, min_num_images=MIN_NUM_IMGS_PER_PERSON)\n",
    "        similar_images.append(similar_images_for_group)\n",
    "        sleep(uniform(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if QUERY_BACKEND:\n",
    "    results = list(itertools.chain.from_iterable(similar_images))\n",
    "    df = pd.DataFrame(results)\n",
    "    # drop duplicate urls and duplicate captions\n",
    "    df = df.drop_duplicates('url', ignore_index=True).drop_duplicates('caption', ignore_index=True)\n",
    "    df.to_csv('cc3m_experiments/laion5b_similar_imgs_to_facescrub.csv')\n",
    "else:\n",
    "    df = pd.read_csv('cc3m_experiments/laion5b_similar_imgs_to_facescrub.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of images per person')\n",
    "df.groupby('name').count().sort_values('caption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "first_100_imgs_per_person = df.groupby('name').head(100)\n",
    "first_100_imgs_per_person[['caption', 'url', 'name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Downlaod the images of the LAION-5B dataset that are most similar to the FaceScrub images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab(uid, caption, url, output_root_folder, actor_name):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/mlfoundations/open_clip/blob/main/src/data/gather_cc.py and adjusted to take caption and url separately.\n",
    "    Download a single image from the TSV.\n",
    "    \"\"\"\n",
    "    output_folder = os.path.join(output_root_folder, actor_name)\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        \n",
    "    file_path = os.path.join(output_folder, f'{uid:04d}.jpg')\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            o = PILImage.open(file_path)\n",
    "            o = np.array(o)\n",
    "\n",
    "            print(\"Finished\", uid, actor_name, url)\n",
    "            return caption, file_path, actor_name\n",
    "        except Exception as e:\n",
    "            print(\"Failed\", uid, actor_name, url, e)\n",
    "            return\n",
    "        \n",
    "\n",
    "    # Let's not crash if anythign weird happens\n",
    "    try:\n",
    "        header = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "        dat = requests.get(url, timeout=20, headers=header)\n",
    "        if dat.status_code != 200:\n",
    "            print(\"404 file\", url)\n",
    "            return\n",
    "\n",
    "        # Try to parse this as an Image file, we'll fail out if not\n",
    "        im = PILImage.open(BytesIO(dat.content))\n",
    "        im.thumbnail((512, 512), PIL.Image.BICUBIC)\n",
    "        if min(*im.size) < max(*im.size)/3:\n",
    "            print(\"Too small\", url)\n",
    "            return\n",
    "\n",
    "        im.save(file_path)\n",
    "\n",
    "        # Another try/catch just because sometimes saving and re-loading\n",
    "        # the image is different than loading it once.\n",
    "        try:\n",
    "            o = PILImage.open(file_path)\n",
    "            o = np.array(o)\n",
    "\n",
    "            print(\"Success\", o.shape, uid, actor_name, url)\n",
    "            return caption, file_path, actor_name\n",
    "        except Exception as e:\n",
    "            print(\"Failed\", uid, actor_name, url, e)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"Unknown error\", e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/workspace/data/conceptual_captions_laion5b')\n",
    "def grab_actor_images(name, captions, urls, root_dir):\n",
    "    res = []\n",
    "    underscore_name = name.replace(\" \", \"_\")\n",
    "    for i, (caption, url) in enumerate(zip(captions, urls)):\n",
    "        res.append(grab(i, caption, url, root_dir, underscore_name))\n",
    "\n",
    "    return res\n",
    "\n",
    "class TQDMParallel(Parallel):\n",
    "    def __init__(self, progress_bar=True, total=None, *args, **kwargs):\n",
    "        self.progress_bar = progress_bar\n",
    "        self.total = total\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        with tqdm(disable=not self.progress_bar, total=self.total) as self.pbar:\n",
    "            return Parallel.__call__(self, *args, **kwargs)\n",
    "\n",
    "    def print_progress(self):\n",
    "        if self.total is None:\n",
    "            self.pbar.total = self.n_dispatched_tasks\n",
    "        self.pbar.n = self.n_completed_tasks\n",
    "        self.pbar.refresh()\n",
    "\n",
    "dfg = first_100_imgs_per_person.groupby('name')\n",
    "results = TQDMParallel(n_jobs=200, total=len(dfg))(\n",
    "    delayed(grab_actor_images)(name, group['caption'], group['url'], 'image_data') for name, group in dfg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chained_results = list(itertools.chain.from_iterable(results))\n",
    "results_df = pd.DataFrame(chained_results, columns=['title', 'filepath', 'class_name'])\n",
    "results_df['name'] = results_df['class_name'].map(lambda x: x.replace(\"_\", \" \") if x else None)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lowest number of pictures for a person:')\n",
    "lowest_imgs_for_person = results_df.groupby('name').count().min()['title']\n",
    "lowest_imgs_for_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the maximum number of downloaded images is x for some individuals, save the first x image text pairs for each person\n",
    "same_num_imgs_per_person = results_df.groupby('name').head(lowest_imgs_for_person)\n",
    "same_num_imgs_per_person.to_csv(f'{lowest_imgs_for_person}_images_per_person_training.csv', sep='\\t', index=False)\n",
    "same_num_imgs_per_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the cc train csv file\n",
    "cc_train = pd.read_csv('../conceptual_captions/Train_GCC-training_output.csv', sep='\\t')\n",
    "cc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members = pd.concat([fs_actors_members, fs_actresses_members])\n",
    "members = pd.merge(same_num_imgs_per_person, members, how='inner', on='name').reset_index()\n",
    "members = members[['title', 'filepath', 'name']]\n",
    "members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a cc csv file with x images for each member\n",
    "NUM_IMAGES_PER_PERSON = [75, 50, 25, 10, 1]\n",
    "for num in NUM_IMAGES_PER_PERSON:\n",
    "    top_members = members.groupby('name').head(num)\n",
    "    top_members[['title', 'filepath']].to_csv(f'top_{num}_images_members_conceptual_captions.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_train['filepath'] = cc_train['filepath'].map(lambda x: '../conceptual_captions/'+x)\n",
    "cc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the images of the persons to the cc train csv\n",
    "for num in NUM_IMAGES_PER_PERSON:\n",
    "    members = pd.read_csv(f'top_{num}_images_members_conceptual_captions.csv', sep='\\t')\n",
    "    combined_df = cc_train.append(members)\n",
    "    # resample the dataframe to shuffle the rows\n",
    "    shuffled_df = combined_df.sample(frac=1, random_state=42)\n",
    "    shuffled_df.to_csv(f'cc_top_{num}_members_train.csv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
